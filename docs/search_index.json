[
["index.html", "퀀트 투자 쿡북: R을 이용한 투자 포트폴리오 만들기 여는 글", " 퀀트 투자 쿡북: R을 이용한 투자 포트폴리오 만들기 이현열 2019-06-13 여는 글 본 책의 목적은 다음과 같습니다. R 내에서 API와 크롤링을 이용하여 주가, 재무제표, 가치지표 등의 데이터를 수집합니다. 팩터 모델을 이용한 종목선정과 포트폴리오 최적화에 대해 알아보도록 합니다 백테스트 및 성과를 평가하도록 합니다. 책 내용의 효율적인 전달을 위해 R 혹은 프로그래밍에 대한 지식이 있는 분을 대상으로 하였습니다. 따라서 R과 R Studio 설치, 기본적인 프로그래밍 내용 등은 배제하였으며, 프로그래밍이 처음 이신분은 해당 내용을 먼저 익히신 후 본 책을 읽으시길 추천드립니다. 시간이 나는대로 작성 후 웹에 업로드 할 예정이며, 모든 내용이 작성완료되면 책으로 퍼블리쉬 할 예정입니다. "],
["section-1.html", "Chapter 1 퀀트 투자의 심장: 데이터와 프로그래밍 1.1 데이터 구하기 1.2 퀀트 투자와 프로그래밍 1.3 R 프로그램 1.4 퀀트 투자에 유용한 R 패키지", " Chapter 1 퀀트 투자의 심장: 데이터와 프로그래밍 몇 년 전까지만 하더라도 퀀트 투자는 일반 투자자들에게 매우 낯선 영역이었던 반면, 최근에는 각종 커뮤니티와 매체를 통해 이제는 익숙한 단어가 되었습니다. 퀀트 투자에서 말하는 퀀트란 모형을 기반으로 금융상품의 가격을 산정하거나, 이를 바탕으로 투자를 하는 사람을 말합니다. 퀀트Quant라는 단어가 계량적을 의미하는 퀀티터티브Quantitative에서 앞 글자를 따온 점을 생각하면 쉽게 이해가 될 것입니다. 퀀트 투자 역시 이와 비슷한 의미입니다. 사람들이 일반적으로 투자법이라 알고 있는 산업과 기업을 분석하여 가치를 매기는 정성적인 투자법과는 달리, 수학과 통계를 기반으로 투자 전략을 만들고 이를 바탕으로 투자하는 정량적인 투자법을 의미합니다. 이처럼 데이터의 수집과 가공, 이를 바탕으로 모델을 만든 후 실행하는 단계는 데이터 사이언스의 업무 흐름도와 매우 유사합니다. 해들리 위컴Hadley Wickham에 따르면1, 데이터 사이언스의 업무 과정은 다음과 같습니다. Figure 1.1: 데이터 사이언스 업무 과정 데이터 사이언스는 프로그래밍을 통해 데이터를 불러온 후 이를 정리하고, 원하는 결과를 찾기 위해 데이터를 변형하거나, 시각화하거나, 모델링을 합니다. 그 후 이러한 결과를 바탕으로 타인과 소통하는 일련의 과정을 의미합니다. 퀀트 투자의 단계 역시 이와 매우 유사합니다. 투자에 필요한 주가, 재무제표 등의 데이터를 수집하여 정리한 후, 필요한 지표를 얻기 위해 가공을 합니다. 그 후 투자 모형을 이용한 투자 종목을 선택하거나 백테스트를 수행하며, 이를 바탕으로 실제 투자를 하고 성과 평가를 하게 됩니다. 따라서 퀀트 투자는 데이터 사이언스가 금융에 응용된 사례라고도 볼 수 있으며, 그 중심에는 역시 데이터와 프로그래밍이 있습니다. 본 책에서도 위의 단계와 동일하게 데이터를 불러오기, 각 데이터 별로 정리하고 가공하기, 시각화를 통해 데이터의 특징 파악하기, 퀀트 모델을 이용하여 종목 선택하기, 백테스트를 실시한 후 성과 및 위험 평가하기에 대해 알아볼 예정입니다. 이에 앞서 본 장에서는 퀀트 투자의 심장이라고 볼 수 있는 데이터는 어떻게 얻을 수 있는지, 왜 프로그래밍을 해야 하는지, 그 중에서도 R은 무엇인지에 대해 간략히 살펴보겠습니다. 1.1 데이터 구하기 퀀트 투자에 필요한 데이터의 경우, 여러 데이터 제공업자들이 제공하는 서비스를 이용한다면 매우 손쉽게 구할 수 있습니다. 글로벌 데이터 수집에는 블룸버그 혹은 Factset, 국내 데이터 수집에는 DataGuide가 흔히 사용됩니다. 물론 비용을 더 지불한다면 단순 데이터 수집뿐만 아니라 즉석에서 백테스트 및 성과 평가까지 가능합니다. Factset에서 판매하는 Alpha Testing 기능, 혹은 S&amp;P Global에서 판매하는 ClariFI®를 사용한다면, 전세계 주식을 대상으로 원하는 전략의 백테스트 결과를 마우스 클릭과 몇 번의 동작만으로 수행할 수 있습니다. Figure 1.2: ClariFI®의 백테스트 기능 이러한 데이터 제공업자들을 이용하는 방법의 최대 단점은 바로 비용입니다. 블룸버그 단말기의 경우 1년 사용료가 대리 한 명의 연봉과 비슷하여, 흔히들 블대리라 부르기도 합니다. 국내 데이터 업체의 경우 이보다 저렴하기는 하지만, 역시 1년 사용료가 수백 만원 정도로, 일반 개인 투자자들이 감당하기에는 부담이 가는 비용입니다. 해외데이터의 경우 Quandl2이나 tiingo3등의 업체가 제공하는 서비스를 통해 상대적으로 저렴한 가격으로 데이터를 구할 수 있기도 합니다. 물론 대형 데이터 제공업체에 비해 데이터의 종류나 기간은 짧은 편이지만, 대부분의 퀀트 투자자들이 주식을 대상으로 한다는 점을 생각해보면 충분한 데이터를 얻을 수 있습니다. tiingo에서는 전세계 64,386개 주식의 30년 이상 가격 정보, 21,352개 주식의 12년 이상 재무정보를 월 $10에 받을 수 있으며, 한정된 종목과 용량에 한해서는 무료로 데이터를 받을 수도 있습니다. 더군다나 API를 통해 프로그램 내에서 직접 데이터를 받을 수 있다는 편의성도 존재합니다. 그러나 아쉽게도 이러한 데이터에서 한국 시장의 정보는 소외가 되어있습니다. 따라서 돈을 들이지 않고 국내 데이터를 얻기 위해서는 직접 발품을 파는 수밖에 없습니다. 야후 파이낸스4에서 제공하는 금융데이터를 API를 통해 받을 수도 있으며, 국내 금융 사이트들에서 제공하는 정보를 크롤링하여 데이터를 가공할 수도 있습니다. Figure 1.3: NAVER 금융 제공 재무정보 아래의 표는 국내의 금융 데이터를 구할 수 있는 사이트의 주소들 입니다. 해당 사이트의 정보를 잘만 활용한다면 장기간의 주가 및 재무정보를 무료로 수집할 수 있습니다. 물론 데이터 제공업체가 제공하는 깔끔한 형태의 데이터가 아니므로 클랜징 작업이 필요하다는 점, 그리고 상장폐지된 기업의 경우 데이터를 구하는 점이 힘들다는 단점이 있습니다. 그러나 비용이 들지 않는 다는 점, 그리고 현재 시점에서 투자 종목을 선택할 때는 상장폐지된 기업이 필요하지 않는다는 점을 고려하면, 이는 큰 문제가 되지 않는다 생각합니다. 1.2 퀀트 투자와 프로그래밍 우리가 구한 데이터는 연구나 투자에 바로 사용할 수 있는 형태로 주어지는 경우가 거의 없기 때문에 이를 목적에 맞게 처리하는 과정을 거쳐야 하며, 이를 흔히 데이터 클랜징 작업이라 합니다. 또한, 데이터가 정제된 이후 이를 활용한 투자 전략의 백테스트나 종목 선정을 위해서도 프로그래밍은 필수입니다. 물론 모든 퀀트 투자에서 프로그래밍이 필수인 것은 아닙니다. 엑셀을 이용하여도 간단한 형태의 백테스트 및 종목 선정은 얼마든지 가능합니다. 그러나 응용성 및 효율성의 측면에서 엑셀을 이용하는 것은 매우 비효율적입니다. 데이터를 수집하고 클랜징 작업을 하는 경우, 몇 종목 되지 않는다면 엑셀을 이용하여도 충분히 가능합니다. 그러나 종목 수가 수 천 종목을 넘어갈 경우, 데이터를 손으로 일일이 처리하는 것은 사실상 불가능에 가깝습니다. 이러한 단순 반복 작업의 경우 프로그래밍을 이용한다면 훨씬 효율적으로 작업을 수행할 수 있습니다. 백테스트에서도 프로그래밍을 사용하는 것이 훨씬 효율적입니다. 과거 12개월 누적 수익률이 높은 30종목에 투자하는 모멘텀 전략의 백테스트를 한다고 가정합시다. 처음에는 엑셀을 통해 백테스트를 하는 것이 편하다고 생각될 수 있습니다. 그러나 만일 12개월이 아닌 6개월 누적 수익률로 백테스트를 하고자 한다면 어떨까요? 다시 6개월 누적 수익률을 구하는 작업을 위해 명령어를 바꾸고 드래그를 해야 할 것입니다. 그러나 프로그래밍을 이용한다면 n = 12 였던 부분을 n = 6으로 변경한 후, 단지 클릭을 하면 새로운 백테스트가 완료됩니다. 전체 데이터가 100MB 정도라 가정할 때, 투자 전략이 계속해서 늘어날 경우는 어떨까요? 엑셀에서 A라는 전략을 백테스트 하기 위해서는 해당 데이터를 이용하여 작업을 한 후 저장합니다. 그 후 B라는 전략을 새롭게 백테스트 하려면 해당 데이터를 새로운 엑셀 파일에 복사하여 작업한 후 저장해야 합니다. 결과적으로 10개의 전략만 백테스트 하더라도 100MB 짜리 엑셀파일이 10개, 즉 1GB 정도의 엑셀 파일이 쌓이게 됩니다. 만일 데이터가 바뀔 경우, 다시 10개 엑셀 시트의 데이터를 일일이 바꿔야 하는 귀찮음도 감수해야 합니다. 물론 하나의 엑셀 파일 내에서 모든 전략을 수행할 수도 있지만, 이는 엄청난 속도 저하의 문제가 있습니다. 그러나 프로그래밍을 이용한다면 어떨까요? 백테스트를 수행하는 프로그래밍 스크립트는 불과 몇 KB에 불과합니다. 10개의 전략에 대한 스크립트 파일을 합해도 1MB가 되지 않습니다. 데이터가 바뀌더라도 원본 데이터 파일 하나만 수정해주면 됩니다. 물론 대부분의 사람들에게 프로그래밍은 매우 낯선 도구입니다. 그러나 퀀트 투자에 필요한 프로그래밍은 매우 한정적이고 반복적이기에, 몇 개의 단어와 구문만 익숙해지면 사용하는데 큰 어려움이 없습니다. 또한 개발자들의 프로그래밍에 비하면 상당히 쉬운 수준이므로, 비교적 빠른 시간 내에 원하는 전략을 테스트하고 수행하는 정도의 능력을 갖출 수도 있습니다. 최근에는 프로그래밍을 공부할 수 있는 좋은 온라인 사이트 및 학원도 늘어나고 있으므로, 프로그래밍을 처음 시작하기에도 큰 어려움이 없습니다. DataCamp5는 사람들이 즐겨 이용하는 온라인 프로그래밍 학원으로써 R, Python, SQL과 같은 언어를 단계별로 배워 나갈 수 있으며, 저럼한 사용료로 모든 과정을 수강할 수 있습니다. 직접 코드를 입력해야 과정이 진행된다는 점에서 초보자들도 능동적으로 학습할 수 있는 장점이 있습니다. 1.3 R 프로그램 인간이 사용하는 언어의 종류가 다양하듯이, 프로그램 언어의 종류 역시 매우 다양합니다. 대략700여개 이상의 프로그램 언어 중6 사람들이 대중적으로 사용하는 언어는 그리 많지 않으므로, 대중성과 효율성을 위해 사용량이 많은 언어를 이용하는 것이 좋습니다. 아래 그림은 프로그래밍 언어의 사용 통계 순위7입니다. 이 중 R과 Python은 금융에서 대표적으로 사용되는 언어로써, 매우 대중적인 언어이기도 합니다. 해당 언어가 많이 사용되는 가장 큰 이유는 무료인 점, 그리고 일반인들도 사용하기에 매우 편한 형태로 언어가 구성되어 있다는 점입니다. Figure 1.4: 2017년 기준 프로그래밍 언어 사용 통계 순위 이러한 프로그래밍 언어 중 본 책에서는 R을 이용하였습니다. R의 장점은 무료라는 점 이외에도, 타 언어는 비교할 수 없는 다양한 패키지 입니다. 두터운 사용자 층을 기반으로 하여 R에는 상상할 수 없을 정도로 다양한 패키지가 존재하며, 특히 통계나 계량분석과 관련된 패키지는 독보적이라 할 수 있습니다. 1.4 퀀트 투자에 유용한 R 패키지 여러 연구자 및 실무자들의 헌신적인 노력과 함께, R에는 금융 연구와 퀀트 투자를 위한 다양한 패키지들이 만들어져 있으며, 누구나 무료로 이용이 가능합니다. 그 중 해당 책의 내용에 사용되는 패키지 중 중요하다고 생각되는 것 위주로 소개하도록 하겠습니다. 각 패키지에 대한 자세한 설명은 구글에서 패키지 명으로 검색한 후, PDF 파일을 통해 확인할 수 있습니다. quantmod: 이름에서 알 수 있듯이 퀀트 투자에 매우 유용한 패키지 입니다. API를 이용하여 데이터를 다운로드 받는 getSymbols() 함수는 너무나 많이 사용되는 함수이며, 이 외에도 볼린져밴드, 이동평균선, 상대 강도 지수RSI 등 여러 기술적 지표들을 주가 차트에 나타낼 수도 있습니다. PerformanceAnalytics: 포트폴리오의 성과와 위험을 측정하는데 역시나 매우 유용한 패키지 입니다. 백테스트에 사용되는 Return.portfolio() 함수는 포트폴리오 단위의 백테스트에 필수적인 함수입니다. 또한 성과를 측정하는 상당히 많은 함수를 이용하여, 포트폴리오의 성과뿐만 아니라 위험을 파악할 수 있습니다. xts: 기본적으로 금융 데이터는 시계열 형태이며, 해당 패키지는 여러 데이터들을 시계열 형태eXtensible Time Series로 변형시켜 줍니다. 일별 수익률을 월별 수익률 혹은 연도별 수익률로 변환하는 apply.monthly()와 apply.yearly() 함수, 데이터들의 특정 시점을 찾아주는 endpoints() 함수 역시 백테스트에 필수적으로 사용되는 함수입니다. 해당 패키지는 PerformanceAnalytics 패키지 설치 시 자동으로 설치됩니다. zoo: 해당 패키지 역시 시계열 데이터를 다루는데 유용함 함수가 존재합니다. rollapply() 함수는 apply() 함수를 전체 데이터가 아닌 롤링-윈도우 기법으로 활용할 수 있게 해주며, na 데이터를 채워주는 na.locf() 함수는 시계열 데이터의 결측치를 보정할 때 매우 유용합니다. httr &amp; rvest: 데이터를 웹에서 직접 수집하기 위해서는 크롤링이 필수이며, httr과 rvest는 이에 사용되는 패키지 입니다. httr의 경우 http의 표준 요청을 수행해주는 패키지로써 단순히 데이터를 받는 GET() 함수와 사용자가 필요한 값을 선택하여 요청하는 POST() 함수가 대표적으로 사용됩니다. rvest의 경우 html 문서의 데이터를 가져오는데 사용되는 패키지이며, 웹 페이지에서 데이터를 크롤링 한 후 원하는 데이터만 뽑는데 필요한 여러 함수들이 포함되어 있습니다. dplyr: 해당 패키지는 데이터 처리에 특화된 패키지로써, R을 이용한 데이터 과학 분야에서 가장 많이 사용되는 패키지 중 하나입니다. C++로 작성되어 매우 빠른 처리속도를 보이며, API나 크롤링을 통해 수집한 데이터들을 정리할 때도 매우 유용하게 사용됩니다. ggplot2: 데이터를 시각화할 때 가장 많이 사용되는 패키지입니다. 물론 R에서 기본적으로 내장된 plot() 함수를 이용하여도 시각화가 가능하지만, 해당 패키지를 이용할 경우 훨씬 다양하고 깔끔하게 데이터를 그림으로 표현할 수 있습니다. R을 활용한 데이터 과학(해들리 위컴, 개럿 그롤문드 저/김설기, 최혜민 역)↩ https://www.quandl.com/↩ https://www.tiingo.com/↩ https://finance.yahoo.com/↩ https://www.datacamp.com/↩ https://en.wikipedia.org/wiki/List_of_programming_languages↩ https://www.tiobe.com/tiobe-index/↩ "],
["section-4.html", "Chapter 2 크롤링을 위한 기본 지식 2.1 인코딩의 이해와 R에서 UTF-8 설정하기 2.2 웹의 동작 방식 2.3 HTML과 CSS 이해하기 2.4 파이프 오퍼레이터 (%&gt;%) 2.5 오류에 대한 예외처리", " Chapter 2 크롤링을 위한 기본 지식 2.1 인코딩의 이해와 R에서 UTF-8 설정하기 2.1.1 인간과 컴퓨터 간 번역의 시작, ASCII R에서 스크립트를 한글로 작성하여 저장한 후 이를 다시 불러올 때, 혹은 한글로된 데이터를 크롤링하면 오류가 뜨거나 읽을 수 없는 문자로 나타나는 경우가 종종 있습니다. 이는 한글 인코딩 때문에 발생하는 문제이며, 이러한 현상을 흔히 인코딩이 깨졌다고 표현합니다. 인코딩이란 사람이 사용하는 언어를 컴퓨터가 사용하는 0과 1로 변환하는 과정을 말하며, 이와 반대의 과정을 디코딩이라고 합니다. 이러한 사람과 컴퓨터간의 번역을 위해 최초로 사용된 방식이 아스키(ASCII: American Standard Code for Information Interchange)입니다. 0부터 127까지 총 128개 바이트에 알파벳과 숫자, 그리고 자주 사용되는 특수문자 값을 부여하고, 글자가 입력되면 이에 대응되는 바이트가 저장됩니다. 그러나 아스키의 American이라는 이름에서 알 수 있듯이 이는 영어의 알파벳이 아닌 다른 언어를 표현하는데는 한계가 있으며, 이를 보완하기 위한 여러 방법들이 나오게 되었습니다. Figure 1.1: 아스키 코드 표 2.1.2 한글 인코딩의 종류 인코딩에 대한 전문적인 내용의 경우 해당 책에서 다룰 분야도 아니며, 한글을 인코딩하는데 쓰이는 EUC-KR과 CP949, 그리고 UTF-8 정도만 알고 넘어가도 충분합니다. 만일 알이라는 단어를 인코딩하기 위해서는 어떠한 방법이 있을까요? 먼저 알이라는 문자 자체에 해당하는 코드를 부여하여 나타내는 방법이 있습니다. 아니면 이를 구성하는 모음과 자음을 나누어 ㅇ, ㅏ, ㄹ 각각에 해당하는 코드를 부여하고 이를 조합할 수도 있습니다. 전자와 같이 완성된 문자 자체로 나타내는 방법을 완성형, 후자와 같이 각 문자로 나타내는 방법을 조합형이라고 합니다. 먼저 한글 인코딩 중 완성형으로 가장 대표적인 방법은 EUC-KR이며, 이는 현대 한글에서 많이 쓰이는 글자 2,350개에 번호를 붙인 방법입니다. 그러나 2,350개 글자로 모든 한글의 조합을 표현하기는 부족하였으며, 이를 보완하고자 마이크로소프트사가 도입한 방법이 CP949 입니다. CP949는 11,720개 한글에 번호를 붙인 방법으로 기존 EUC-KR보다 나타낼 수 있는 한글의 갯수가 훨씬 많아졌습니다. 윈도우의 경우 기본 인코딩이 CP949로 되어 있습니다. 조합형의 대표적 방법으로는 UTF-8이 있습니다. 이는 모음과 자음 각각에 코드를 부여한 후 조합하여 한글을 나타냅니다. 조합형의 경우 한글뿐만이 아니라 다양한 언어에 적용할 수 있다는 장점으로 인해 전세계 웹페이지의 대부분이 UTF-8로 만들어 지고 있습니다. Figure 1.2: 웹페이지에서 사용되는 인코딩 비율 2.1.3 R에서 UTF-8 설정하기 위에서 언급했듯이 윈도우에서는 기본 인코딩이 CP949로 이루어져 있으며, 일부 국내 홈페이지는 EUC-KR로 인코딩이 된 경우도 있습니다. 반면 R의 여러 함수들은 인코딩이 UTF-8로 이루어져 있기에, 이러한 인코딩 방식의 차이로 인해 스크립트 작성 및 크롤링 과정에서 오류가 발생하는 경우가 종종 있습니다. 윈도우 컴퓨터에서는 CP949가 기본으로 설정되어 있습니다. 만일 CP949 인코딩을 그대로 사용할 경우, 미리 저장되었던 한글 스크립트가 깨져 나오는 일이 발생할 수 있습니다. 이를 위해 기본 인코딩을 UTF-8로 변경해주는 것이 좋습니다. R Studio의 Tools → Global Options 메뉴에서 Code → Saving 항목 중 Default text encodings 항목을 통해 기본 인코딩을 UTF-8로 변경해주도록 합니다. Figure 1.3: 인코딩 설정 항목 해당 방법으로도 해결되지 않을 경우 File → Reopen with Encoding 메뉴에서 UTF-8 항목을 선택, Set as default encoding for source files 항목을 선택한 후 OK를 누르면 UTF-8로 인코딩이 설정된 후 파일을 다시 열게 됩니다. Figure 1.4: 인코딩 설정 항목 2.2 웹의 동작 방식 크롤링은 웹사이트의 정보를 수집하는 과정이니 만큼, 웹이 어떻게 동작하는지 알아둘 필요가 있습니다. Figure 2.1: 웹 환경 구조 먼저 클라이언트는 여러분의 데스크탑이나 휴대폰과 같은 장치, 그리고 이런 장치의 크롬이나 파이어폭스와 같은 소프트웨어를 의미합니다. 반대로 서버는 웹사이트, 앱을 저장하는 컴퓨터를 의미합니다. 클라이언트가 특정 정보를 요구하는 과정을 요청이라 하며, 서버가 해당 정보를 제공하는 과정을 응답 이라고 합니다. 그러나 클라이언트와 서버가 연결되어 있지 않다면 둘 간에 정보를 주고 받는 것은 불가능하며, 이를 연결해주는 공간이 바로 인터넷입니다. 또한 건물에도 고유의 주소가 있는 것처럼, 각 서버에도 고유의 주소가 있으며, 이것이 인터넷주소 혹은 URL 입니다. 여러분이 네이버에서 경제 기사를 클릭하는 경우를 생각해 봅시다. 클라이언트는 사용자인 여러분, 서버는 네이버이며, URL은 www.naver.com 이 됩니다. 경제 기사를 클릭하는 과정이 요청이며, 클릭 후 해당 페이지를 보여주는 과정이 응답입니다. 2.2.1 HTTP 클라이언트가 각기 다른 방법으로 데이터를 요청한다면, 서버는 해당 요청을 알아듣지 못할 것입니다. 이를 방지하기 위해 규정된 약속이나 표준에 맞추어 데이터를 요청해야하며, 이러한 약속을 HTTP(HyperText Transfer Protocol)라 합니다. 클라이언트가 서버에게 요청의 목적이나 종류를 알리는 방법을 HTTP 요청 방식(HTTP Request Method)이라고 합니다. 이는 크게 GET, POST, PUT, DELETE 4가지로 나눌 수 있지만 크롤링에는 GET과 POST 방식이 대부분 사용되므로 이 두가지만 아는 것도 충분합니다. GET 방식과 POST 방식에 대한 차이 및 크롤링 방법은 데이터 수집 파트에서 자세하게 다루도록 하겠습니다. Table 2.1: HTTP 요청 방식과 설명 요청방식 주소 GET 특정 정보 조회 POST 새로운 정보 등록 PUT 기존 특정 정보 갱신 DELETE 기존 특정 정보 삭제 인터넷을 사용하다 보면 한번쯤 이 페이지를 볼 수 있는 권한이 없습니다.(HTTP 오류 403 - 사용할 수 없음) 혹은 페이지를 찾을 수 없음(HTTP 오류 404 - 파일을 찾을 수 없음) 이라는 오류가 발생한 적이 있을 겁니다. 여기서 403과 404이라는 숫자는 클라이언트의 요청에 대한 서버의 응답 상태를 나타내는 코드이며, 이를 HTTP 상태 코드라 합니다. HTTP 상태 코드는 100번대 부터 500번대 까지 있으며, 성공적으로 응답을 받을 시 200번 코드를 받게 됩니다. 각 코드에 대한 내용은 HTTP 상태 코드를 검색하면 확인할 수 있으며, 크롤링 과정에서 오류가 발생할 시 해당 코드를 통해 어떤 부분에서 오류가 발생하였는지 확인이 가능합니다. Table 2.2: HTTP 상태 코드 그룹 별 내용 코드 주소 내용 1xx Informational (조건부 응답) 리퀘스트를 받고, 처리 중에 있음 2xx Success (성공) 리퀘스트를 정상적으로 처리함 3xx Redirection (리디렉션) 리퀘스트 완료를 위해 추가 동작이 필요함 4xx Client Error (클라이언트 오류) 클라이언트 요청을 처리할 수 없어 오류 발생 5xx Server Error (서버 오류) 서버에서 처리를 하지 못하여 오류 발생 2.3 HTML과 CSS 이해하기 클라이언트와 서버가 데이터를 주고 받을때는 디자인이라는 개념이 필요하지 않습니다. 그러나 응답 받은 정보를 사람이 확인하기 위해서는 보기 편한 방식으로 바꾸어 줄 필요가 있으며, 웹페이지가 그러한 역할을 합니다. 웹페이지의 제목, 단락, 목록 등 레이아웃을 잡아주는데 쓰이는 대표적인 마크업 언어가 HTML(HyperText Markup Language) 입니다. HTML을 통해 잡혀진 뼈대에 글자의 색상이나 폰트, 배경 색, 배치 등 화면을 꾸며주는 역할을 하는 것이 CSS(Cascading Style Sheets) 입니다. 우리의 목적은 웹페이지를 만드는 것이 아니기에 HTML과 CSS에 대해 지나치게 자세히 알 필요는 없습니다. 그러나 크롤링 하고자 하는 데이터가 페이지의 어떤 태그 내에 위치하고 있는지, 어떻게 크롤링을 하면 될지 파악하기 위해서는 HTML과 CSS에 대한 기본적인 지식은 알아둘 필요가 있습니다. 2.3.1 HTML 기본 구조 HTML은 크게 메타 데이터를 나타내는 &lt;head&gt; 부분과 본문을 나타내는 &lt;body&gt; 부분으로 나누어집니다. &lt;head&gt;에서 &lt;title&gt;은 웹페이지에서 나타나는 제목을 나타내며 &lt;body&gt; 내에는 본문에 들어갈 각종 내용들이 포함되어 있습니다. &lt;html&gt; &lt;head&gt; &lt;title&gt;Page Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h2&gt; This is page heading &lt;/h2&gt; &lt;p&gt; THis is first paragraph text &lt;/p&gt; &lt;/body&gt; &lt;/html&gt; Figure 2.2: HTML 기본 구조 2.3.2 태그와 속성 HTML 코드는 태그와 속성, 그리고 내용으로 이루어져 있습니다. 크롤링한 데이터에서 특정 태그의 데이터만을 찾는 방법, 특정 속성의 데이터만을 찾는 방법, 뽑힌 자료에서 내용만을 찾는 방법등의 내용을 찾는 방법이 모두 다르기 때문에 이러한 요소에 대해 좀더 자세히 살펴보도록 하겠습니다. Figure 2.3: HTML 구성 요소 분석 꺽쇠(&lt;&gt;)로 감싸져 있는 부분을 태그라 부르며, 여는 태그 &lt;&gt;가 있으면 반드시 이를 닫아주는 태그인 &lt;/&gt;가 쌍으로 존재해야 합니다. 속성은 해당 태그에 대한 추가적인 정보를 제공해주는 것으로써, 뒤에 속성값이 따라와야 합니다. 내용은 우리가 눈으로 보는 텍스트 부분을 의미합니다. 위의 HTML 코드는 문단을 나타내는 &lt;p&gt; 태그, 정렬을 나타내는 align 속성과 center를 통해 가운데 정렬을, 내용에는 ‘퀀트 투자 Cookbook을, 그리고 태그를 &lt;/p&gt;를 통해 태그를 마쳤습니다. 2.3.3 h 태그와 p 태그 h 태그는 폰트의 크기를 나타내는 태그이며, p 태그는 문단을 나타내는 태그입니다. 이를 사용한 간단한 예제는 다음과 같습니다. h태그의 숫자가 작을수록 텍스트의 크기는 커지는 것이 확인되며, 숫자는 1에서 6까지 지원이 됩니다. p 태그를 사용할 경우 각각의 문단이 만들어지는 것이 확인됩니다. &lt;html&gt; &lt;body&gt; &lt;h1&gt;Page heading: size 1&lt;/h1&gt; &lt;h2&gt;Page heading: size 2&lt;/h2&gt; &lt;h3&gt;Page heading: size 3&lt;/h3&gt; &lt;p&gt;Quant Cookbook&lt;/p&gt; &lt;p&gt;By Henry&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; Figure 2.4: h 태그와 p 태그 예제 2.3.4 리스트: ul과 ol 태그 ul과 ol태그는 리스트(글머리 기호)를 만들 때 사용되며, ul은 경우 순서가 없는(unordered list), ol의 경우 순서가 있는(ordered list)를 만듭니다. &lt;html&gt; &lt;body&gt; &lt;h2&gt; Unordered List&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Price&lt;/li&gt; &lt;li&gt;Financial Statement&lt;/li&gt; &lt;li&gt;Sentiment&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt; Ordered List&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;Import&lt;/li&gt; &lt;li&gt;Tidy&lt;/li&gt; &lt;li&gt;Understand&lt;/li&gt; &lt;li&gt;Communicate&lt;/li&gt; &lt;/ol&gt; &lt;/body&gt; &lt;/html&gt; Figure 2.5: 리스트 관련 태그 예제 ul 태그로 감싸진 부분은 글머리 기호가 순서가 없는 •으로 표현되었으며, ol 태그로 감싸진 부분은 숫자가 순서대로 표현되었습니다. 각각의 리스트는 li를 통해 생성하게 됩니다. 2.3.5 table 태그 table 태그는 표를 만드는 태그입니다. &lt;html&gt; &lt;body&gt; &lt;h2&gt;Major Stock Indices and US ETF&lt;/h2&gt; &lt;table&gt; &lt;tr&gt; &lt;th&gt;Country&lt;/th&gt; &lt;th&gt;Index&lt;/th&gt; &lt;th&gt;ETF&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;US&lt;/td&gt; &lt;td&gt;S&amp;P 500&lt;/td&gt; &lt;td&gt;IVV&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Europe&lt;/td&gt; &lt;td&gt;Euro Stoxx 50&lt;/td&gt; &lt;td&gt;IEV&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Japan&lt;/td&gt; &lt;td&gt;Nikkei 225&lt;/td&gt; &lt;td&gt;EWJ&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Korea&lt;/td&gt; &lt;td&gt;KOSPI 200&lt;/td&gt; &lt;td&gt;EWY&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/body&gt; &lt;/html&gt; Figure 2.6: table 태그 예제 table 태그 내의 tr 태그는 각 행을 의미합니다. 각 셀의 구분은 th 혹은 td 태그를 통해 구분이 가능하며, th 태그는 진하게 표현되므로 주로 테이블의 제목에, td 태그는 테이블의 내용에 사용됩니다. 2.3.6 a, src 태그와 속성 a 태그와 src 태그는 다른 태그와는 다르게, 혼자 쓰이기 보다는 속성과 결합하여 사용됩니다. 먼저 a 태그는 href 속성과 결합하여 다른 페이지의 링크를 걸 수 있습니다. src 태그는 img 속성과 결합하여 이미지를 불러옵니다. &lt;html&gt; &lt;body&gt; &lt;h2&gt;a tag &amp; href attribute&lt;/h2&gt; &lt;p&gt;HTML links are defined with the a tag. The link address is specified in the href attribute:&lt;/p&gt; &lt;a href=&quot;https://henryquant.blogspot.com/&quot;&gt;Henry&#39;s Quantopia&lt;/a&gt; &lt;h2&gt;img tag &amp; src attribute&lt;/h2&gt; &lt;p&gt;HTML images are defined with the img tag, and the filename of the image source is specified in the src attribute:&lt;/p&gt; &lt;img src=&quot;https://cran.r-project.org/Rlogo.svg&quot;, width=&quot;180&quot; height=&quot;140&quot;&gt; &lt;/body&gt; &lt;/html&gt; Figure 2.7: a 태그와 src 태그 예제 a 태그 뒤 href 속성에 대한 속성값으로 연결하고자 하는 웹페이지의 주소를 입력한 후, 내용을 입력하면, 해당 텍스트에 웹페이지의 링크가 추가됩니다. img 태그 뒤 src 속성에는 불러오고자 하는 이미지의 주소를 입력하며, width 속성과 height 속성을 통해 가로와 세로 길이를 조절할 수도 있습니다. 페이지 내에서 링크된 주소를 모두 찾거나, 혹은 모든 이미지를 저장하는 작업을 하고자 할 시, 이러한 속성값을 찾으면 손쉽게 원하는 작업을 할 수 있습니다. 2.3.7 div 태그 div 태그는 화면의 전체적인 틀(레이아웃)을 만들 때 주로 사용하는 태그입니다. 단독으로도 사용될 수 있으며, 꾸밈을 담당하는 style 속성과 결합되어 사용되기도 합니다. &lt;html&gt; &lt;body&gt; &lt;div style=&quot;background-color:black;color:white&quot;&gt; &lt;h5&gt;First Div&lt;/h2&gt; &lt;p&gt;Black backgrond, White Color&lt;/p&gt; &lt;/div&gt; &lt;div style=&quot;background-color:yellow;color:red&quot;&gt; &lt;h5&gt;Second Div&lt;/h2&gt; &lt;p&gt;Yellow backgrond, Red Color&lt;/p&gt; &lt;/div&gt; &lt;div style=&quot;background-color:blue;color:grey&quot;&gt; &lt;h5&gt;Second Div&lt;/h2&gt; &lt;p&gt;Blue backgrond, Grey Color&lt;/p&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; Figure 2.8: div 태그 예제 div 태그를 통해 총 3개의 레이아웃으로 나누어졌음이 확인됩니다. style 속성 중 background-color는 배경 색상을, color는 글자 색상을 의미하며, 각 레이아웃 마다 다른 스타일이 적용되었습니다. 2.3.8 CSS CSS는 앞서 설명했듯이 웹페이지를 꾸며주는 역할을 합니다. head 부분에서 각 태그에 CSS 효과를 입력할 경우, 본문의 해당 태그들은 모두 CSS 효과가 적용됩니다. 이처럼 페이지를 꾸미기 위해 특정 요소에 접근하는 것을 셀렉터Selector라 합니다. &lt;html&gt; &lt;head&gt; &lt;style&gt; body {background-color: powderblue;} h1 {color: blue;} p {color: red;} &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;This is a heading&lt;/h1&gt; &lt;p&gt;This is a first paragraph.&lt;/p&gt; &lt;p&gt;This is a second paragraph.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; Figure 2.9: css 예제 head 태그 사이에 여러 태그에 대한 CSS 효과가 정의되었습니다. 먼저 body의 전체 배경색상을 파우더 블루로 주었으며, h1 태그의 글씨는 파란색으로, p 태그의 글씨는 붉은색으로 주었습니다. body 태그 내에서 style을 태그를 주지 않더라도, CSS 효과가 모두 적용되었음이 확인됩니다. 2.3.9 class와 id CSS를 이용할 경우 본문의 모든 태그에 효과가 적용되므로, 특정한 요소Element에만 동일한 효과를 적용할 수 없습니다. 클래스 속성을 이용할 경우 동일한 이름을 가진 클래스에는 동일한 효과가 적용됩니다. &lt;html&gt; &lt;style&gt; .index { background-color: tomato; color: white; padding: 10px; } .desc { background-color: moccasin; color: black; padding: 10px; } &lt;/style&gt; &lt;div&gt; &lt;h2 class=&quot;index&quot;&gt;S&amp;P 500&lt;/h2&gt; &lt;p class=&quot;desc&quot;&gt; Market capitalizations of 500 large companies having common stock listed on the NYSE, NASDAQ, or the Cboe BZX Exchange&lt;/p&gt; &lt;/div&gt; &lt;div&gt; &lt;h2&gt;Dow Jones Industrial Average&lt;/h2&gt; &lt;p&gt;Value of 30 large, publicly owned companies based in the United States&lt;/p&gt; &lt;/div&gt; &lt;div&gt; &lt;h2 class=&quot;index&quot;&gt;NASDAQ Composite&lt;/h2&gt; &lt;p class=&quot;desc&quot;&gt;The composition of the NASDAQ Composite is heavily weighted towards information technology companies&lt;/p&gt; &lt;div&gt; &lt;/html&gt; Figure 2.10: class 예제 셀렉터를 클래스에 적용할때는 클래스명 앞에 콤마(.)를 붙혀 표현합니다. 위의 예제에서 index 클래스는 배경 색상이 토마토, 글씨는 흰색, 여백은 10px로 정의되었습니다. desc 클래스는 배경 색상이 모카신, 글씨는 검은색, 여백은 10px로 정의되었습니다. 본문의 첫번째와 세번째 레이아웃에만h2 태그 뒤에는 ‘index’ 클래스를, p 태그 뒤에는 ‘desc’ 클래스를 속성으로 입력하였습니다. 따라서 해당 레이아웃에만 CSS 효과가 적용되며, 클래스 값이 없는 두번째 레이아웃에는 효과가 적용되지 않습니다. id 또한 이와 비슷한 역할을 하며, HTML 내에서 여러 개의 class가 정의될 수 있는 반면, id는 단 하나만 사용하기를 권장합니다. &lt;html&gt; &lt;head&gt; &lt;style&gt; /* Style the element with the id &quot;myHeader&quot; */ #myHeader { background-color: lightblue; color: black; padding: 15px; text-align: center; } &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;!-- A unique element --&gt; &lt;h1 id=&quot;myHeader&quot;&gt;My Header&lt;/h1&gt; &lt;/body&gt; &lt;/html&gt; Figure 2.11: id 예제 셀렉터를 id에 적용할때는 클래스명 앞에 샵(#)를 붙혀 표현하며, 페이지에서 한 번만 사용된다는 점을 제외하면 클래스와 사용방법이 거의 동일합니다. 클래스나 id 값을 통해 원하는 내용을 크롤링 하는 경우도 많으므로, 각각의 이름 앞에 콤마(.)와 샵(#)을 붙여야 한다는 점을 꼭 기억하시기 바랍니다. 크롤링에 필요한 HTML 관련 지식은 위에서 언급한 정도로도 충분하다고 생각됩니다. 추가적인 정보가 필요하거나 내용이 궁금하신 분들은 아래 사이트를 참고하기 바랍니다. w3schools: https://www.w3schools.in/html-tutorial/ 웨버 스터디: http://webberstudy.com/ 2.4 파이프 오퍼레이터 (%&gt;%) R 내에서 동일한 데이터를 대상으로 연속적으로 작업하게 해주는 오퍼레이터(연산자)가 바로 파이프 오퍼레이터 입니다. 크롤링에 꼭 필요한 rvest 패키지를 설치할 경우 자동으로 magrittr 패키지가 설치되어 사용할 수 있습니다. 흔히 프로그래밍에서 x라는 데이터를 F()라는 함수에 넣어 결과값을 확인하고 싶을 경우, F(x)의 방법을 사용합니다. 예를 들어 3과 5라는 데이터 중 큰 값을 찾고 싶을 때는 max(3,5)를 통해 확인합니다. 이를 통해 나온 결과 값을 또 다시 G()라는 함수에 넣어 결과값을 확인하고자 할 경우, 비슷한 과정을 거칩니다. max(3,5)를 통해 나온 값의 제곱근을 구하고자 할 경우 result = max(3,5)를 통해 첫 번째 결과값을 저장하고, sqrt(result)를 통해 두 번째 결과값을 계산합니다. 물론 sqrt(max(3,5))와 같은 표현법으로 한번에 표현할 수 있습니다. 이러한 표현의 단점은, 계산하는 함수가 많아질수록 저장하는 변수가 늘어나거나 혹은 괄호가 지나치게 길어집니다. 그러나 파이프 오퍼레이터인 %&gt;%를 사용할 경우, 함수 간의 관계를 매우 직관적으로 표현하고 이해할 수 있습니다. 이를 정리하면 아래 표와 같습니다. Table 2.3: 파이프 오퍼레이터의 표현과 내용 비교 내용 주소 F(x) x %&gt;% F G(F(x)) x %&gt;% F %&gt;% G 다음으로 파이프 오퍼레이터의 간단한 예제 를 통해 사용법을 살펴보도록 하겠습니다. 먼저 다음과 같은 10개의 숫자가 있다고 가정합니다. x = c(0.3078, 0.2577, 0.5523, 0.0564, 0.4685, 0.4838, 0.8124, 0.3703, 0.5466, 0.1703) 우리가 원하는 과정은 각 값들의 로그값을 구할 것 로그값들의 계차를 구할 것 구해진 계차의 지수값을 구할 소수 둘째 자리까지 반올림할 것 입니다. 즉 log(), diff(), exp(), round()에 대한 값을 순차적으로 구하고자 합니다. x1 = log(x) x2 = diff(x1) x3 = exp(x2) round(x3, 2) ## [1] 0.84 2.14 0.10 8.31 1.03 1.68 0.46 1.48 0.31 첫 번째 방법은, 단계 별 함수의 결과값을 변수에 저장하고, 저장된 변수를 다시 불러와 함수에 넣고 계산하는 방법입니다. 전반적인 계산 과정을 확인하기에는 좋지만, 매번 변수에 저장하고 불러오고 하는 과정은 매우 비효율 적이며, 코드 또한 불필요하게 길어집니다. round(exp(diff(log(x))), 2) ## [1] 0.84 2.14 0.10 8.31 1.03 1.68 0.46 1.48 0.31 두 번째는 괄호를 통해 감싸는 방법입니다. 앞선 방법에 비해 코드는 짧아졌지만, 계산 과정을 알아보기에는 매우 불편한 방법으로 코드가 짜여 있습니다. library(magrittr) x %&gt;% log() %&gt;% diff() %&gt;% exp() %&gt;% round(., 2) ## [1] 0.84 2.14 0.10 8.31 1.03 1.68 0.46 1.48 0.31 마지막으로 파이프 오퍼레이터를 사용하는 방법입니다. 코드도 짧으며, 계산 과정을 한눈에 파악하기도 좋습니다. 맨 왼쪽에는 원하는 변수를 입력하며, %&gt;% 뒤에는 차례대로 계산하고자 하는 함수를 입력합니다. 변수의 입력값을 ()로 비워둘 경우, 오퍼레이터의 좌측에 있는 값이 입력변수가 됩니다. 반면 round()와 같이 입력값이 2개 이상 필요할 경우, 콤마(.)가 오퍼레이터의 좌측 값으로 입력됩니다. 파이프 오퍼레이터는 크롤링 뿐만이 아닌, 모든 코드에 사용할 수 있습니다. 이를 통해 훨씬 깔끔하면서도 데이터 처리과정을 직관적으로 이해할 수 있습니다. 2.5 오류에 대한 예외처리 크롤링을 이용하여 데이터를 수집할 경우, 일반적으로 for loop 구문을 통해 수천 종목에 해당하는 웹페이지에 접속하여 해당 데이터를 읽어옵니다. 그러나 특정 종목에 해당하는 페이지가 존재하지 않거나, 혹은 단기적으로 접속이 불안정할 경우 오류가 발생하여 루프를 처음부터 다시 실행해야 하는 번거로움이 있습니다. 이러할 경우 R에서는 tryCatch() 함수를 이용하여 예외처리, 즉 오류가 발생할 경우 이를 무시하고 넘어갈 수 있습니다. tryCatch() 함수의 구조는 다음과 같습니다. result = tryCatch({ expr }, warning = function(w) { warning-handler-code }, error = function(e) { error-handler-code }, finally = { cleanup-code }) 먼저 expr는 실행하고자 하는 코드를 의미합니다. warning은 경고를 나타내며, warning-handler-code는 경고 발생시 실행할 구문을 의미합니다. 이와 비슷하게, error와 error-handler-code는 각각 오류와 오류 발생시 실행할 구문을 의미합니다. finally는 오류의 여부와 관계없이 무조건 수행할 구문을 의미하며, 이는 생략이 가능합니다. number = data.frame(1,2,3,&quot;4&quot;,5, stringsAsFactors = FALSE) str(number) ## &#39;data.frame&#39;: 1 obs. of 5 variables: ## $ X1 : num 1 ## $ X2 : num 2 ## $ X3 : num 3 ## $ X.4.: chr &quot;4&quot; ## $ X5 : num 5 먼저 number 변수에는 1에서 5까지 값이 입력되어 있으며, 다른 값들은 형태가 숫자인 반면, 4는 문자 형태입니다. for (i in number) { print(i^2) } ## [1] 1 ## [1] 4 ## [1] 9 ## Error in i^2: 이항연산자에 수치가 아닌 인수입니다 for loop 구문을 통해 순서대로 값들의 제곱을 출력하는 명령어를 실행할 경우, 문자 4는 제곱을 할 수 없어 오류가 발생하게 됩니다. tryCatch() 함수를 사용할 경우, 이처럼 오류가 발생하는 루프를 무시하고 다음 루프로 넘어갈 수 있게 됩니다. for (i in number) { tryCatch({ print(i^2) }, error = function(e) { print(paste(&#39;Error:&#39;, i)) }) } ## [1] 1 ## [1] 4 ## [1] 9 ## [1] &quot;Error: 4&quot; ## [1] 25 expr부분은 print(i^2) 이며, error-handler-code 부분은 print(paste('Error:', i)), 즉 오류가 발생한 i를 출력하는 것입니다. 해당 코드를 실행할 경우 문자 4에서 오류가 발생함을 알려준 후, 루프가 멈추지 않고 다음으로 진행됩니다. "],
["api-.html", "Chapter 3 API를 이용한 데이터 수집 3.1 API를 이용한 Quandl 데이터 다운로드 3.2 getSymbols() 함수를 이용한 API 다운로드", " Chapter 3 API를 이용한 데이터 수집 본 장과 다음 장에서는 본격적으로 데이터를 수집하는 방법에 대해 배우도록 하겠습니다. 그 중 해당 장에서는 API를 이용하여 데이터를 수집하는 방법에 대해 살펴보도록 합니다. API 제공자는 본인이 가진 데이터베이스를 다른 누군가가 쉽게 사용할 수 있는 형태로 가지고 있으며, 해당 데이터베이스에 접근할 수 있는 열쇠인 API 주소를 가진 사람은 이를 언제든지 사용할 수 있습니다. Figure 1.1: API 개념 API 주소만 가지고 있다면 데이터를 언제, 어디서, 누구나 쉽게 이용할 수 있다는 장점이 있습니다. 또한 대부분의 경우 사용자가 필요한 데이터만을 가지고 있으므로 접속 속도가 빠르며, 데이터를 가공하는 번거로움도 줄어듭니다. 해외의 경우 금융 데이터를 API의 형태로 제공하는 업체가 많으므로, 이를 잘만 활용한다면 매우 손쉽게 퀀트 투자에 필요한 데이터를 수집할 수 있습니다. 3.1 API를 이용한 Quandl 데이터 다운로드 데이터 제공업체 Quandl은 일부 데이터를 무료로 제공하며, API를 통해서 이를 다운로드 받을 수 있습니다.8 해당 책에서는 예제로써 애플(AAPL)의 주가를 다운로드 받도록 하겠습니다. csv 형식의 API 주소는 다음과 같습니다. https://www.quandl.com/api/v3/datasets/WIKI/AAPL/data.csv?api_key=xw3NU3xLUZ7vZgrz5QnG 위 주소를 웹 브라우저 주소 창에 직접 입력하면 csv 형식의 파일이 다운로드 되며, 이를 열어보면 애플의 주가에 해당하는 정보들이 다운로드 됨이 확인됩니다. Figure 1.2: API 주소를 이용한 데이터 다운로드 그러나 웹 브라우저에 해당 주소를 입력하여 csv 파일을 다운로드 받고, 이를 다시 R에서 불러오는 작업은 무척이나 비효율 적입니다. R 내에서 API 주소를 이용하여 직접 데이터를 다운로드 받아올 수 있습니다. url.aapl = &quot;https://www.quandl.com/api/v3/datasets/WIKI/AAPL/ data.csv?api_key=xw3NU3xLUZ7vZgrz5QnG&quot; data.aapl = read.csv(url.aapl) head(data.aapl) ## Date Open High Low Close Volume Ex.Dividend Split.Ratio ## 1 2018-03-27 173.68 175.15 166.92 168.340 38962839 0 1 ## 2 2018-03-26 168.07 173.10 166.44 172.770 36272617 0 1 ## 3 2018-03-23 168.39 169.92 164.94 164.940 40248954 0 1 ## 4 2018-03-22 170.00 172.68 168.60 168.845 41051076 0 1 ## 5 2018-03-21 175.04 175.09 171.26 171.270 35247358 0 1 ## 6 2018-03-20 175.24 176.80 174.94 175.240 19314039 0 1 ## Adj..Open Adj..High Adj..Low Adj..Close Adj..Volume ## 1 173.68 175.15 166.92 168.340 38962839 ## 2 168.07 173.10 166.44 172.770 36272617 ## 3 168.39 169.92 164.94 164.940 40248954 ## 4 170.00 172.68 168.60 168.845 41051076 ## 5 175.04 175.09 171.26 171.270 35247358 ## 6 175.24 176.80 174.94 175.240 19314039 url1에 해당 주소를 입력한 후, read.csv() 함수를 이용하여 간단하게 csv 파일을 불러올 수 있습니다. API 주소를 조금만 수정하면, 다른 종목의 데이터도 다운로드 받을 수 있습니다. 위의 주소에서 애플의 티커에 해당하는 AAPL를 넷플렉스의 티커에 해당하는 NFLX로 변경하여 데이터를 다운로드 받아보도록 하겠습니다. url.nflx = &#39;https://www.quandl.com/api/v3/datasets/WIKI/NFLX/ data.csv?api_key=xw3NU3xLUZ7vZgrz5QnG&#39; data.nflx = read.csv(url.nflx) head(data.nflx) ## Date Open High Low Close Volume Ex.Dividend Split.Ratio ## 1 2018-03-27 322.49 322.90 297.000 300.69 11890994 0 1 ## 2 2018-03-26 309.36 321.03 302.000 320.35 11906279 0 1 ## 3 2018-03-23 307.41 310.73 300.360 300.94 9226978 0 1 ## 4 2018-03-22 313.07 314.12 305.660 306.70 7920524 0 1 ## 5 2018-03-21 316.35 319.40 314.511 316.48 5016980 0 1 ## 6 2018-03-20 313.26 319.50 312.800 317.50 5922296 0 1 ## Adj..Open Adj..High Adj..Low Adj..Close Adj..Volume ## 1 322.49 322.90 297.000 300.69 11890994 ## 2 309.36 321.03 302.000 320.35 11906279 ## 3 307.41 310.73 300.360 300.94 9226978 ## 4 313.07 314.12 305.660 306.70 7920524 ## 5 316.35 319.40 314.511 316.48 5016980 ## 6 313.26 319.50 312.800 317.50 5922296 넷플릭스의 주가 역시 손쉽게 다운로드 받을 수 있음이 확인됩니다. 3.2 getSymbols() 함수를 이용한 API 다운로드 앞선 예에서 API 주소를 이용할 경우 매우 간단하게 데이터를 수집할 있음을 살펴 보았습니다. 그러나 해당 방법에는 여러 단점 또한 존재합니다. 먼저, 원하는 항목에 대한 API를 일일이 얻는 것이 힘든 일입니다. 또한 Quandl의 경우 무료로 얻을 수 있는 정보에 제한이 있으며, 다운로드 양에 대한 제한도 있습니다. 한 두 종목의 경우 해당 방법으로 데이터를 수집할 수 있지만, 전 종목의 데이터를 해당 방법으로 구하는 것은 사실상 불가능 합니다. 다행히 야후 파이낸스에서도 주가 데이터를 무료로 제공하며, quantmod 패키지의 getSymbols() 함수는 해당 API에 접속하여 데이터를 다운로드 받아옵니다. 3.2.1 주가 다운로드 getSymbols() 함수의 기본적인 사용법은 매우 간단합니다. 괄호 안에 다운로드 받고자 하는 종목의 티커를 입력하면 됩니다. 아래의 코드는 애플의 티커인 “AAPL”을 입력한 경우입니다. 티커와 동일한 변수인 AAPL이 생성되며, 주가 데이터가 다운로드 된 후 xts 형태로 입력됩니다. library(quantmod) getSymbols(&#39;AAPL&#39;) ## [1] &quot;AAPL&quot; head(AAPL) ## AAPL.Open AAPL.High AAPL.Low AAPL.Close AAPL.Volume ## 2007-01-03 12.32714 12.36857 11.70000 11.97143 309579900 ## 2007-01-04 12.00714 12.27857 11.97429 12.23714 211815100 ## 2007-01-05 12.25286 12.31428 12.05714 12.15000 208685400 ## 2007-01-08 12.28000 12.36143 12.18286 12.21000 199276700 ## 2007-01-09 12.35000 13.28286 12.16429 13.22429 837324600 ## 2007-01-10 13.53571 13.97143 13.35000 13.85714 738220000 ## AAPL.Adjusted ## 2007-01-03 7.951960 ## 2007-01-04 8.128461 ## 2007-01-05 8.070577 ## 2007-01-08 8.110430 ## 2007-01-09 8.784165 ## 2007-01-10 9.204537 다운로드 결과로써 총 6개의 열이 생성됩니다. Open은 시가, High는 고가, Low는 저가, Close는 종가를 의미합니다. 또한, Volume은 거래량을 의미하며, Adjusted는 배당이 반영된 수정주가를 의미합니다. 이 중 가장 많이 사용되는 데이터는 Adjusted, 즉 배당이 반영된 수정주가입니다. Ad() 함수는 getSymbols() 함수를 통해 다운로드 받은 데이터에서 수정주가만을 선택하여 줍니다. 시계열 그래프를 그려주는 chart_Series()와 함께 수정주가를 그리면 다음과 같습니다. chart_Series(Ad(AAPL)) 시계열 기간을 입력하지 않을 경우 2007년 1월부터 현재까지의 데이터가 다운로드 되며, 추가적인 입력 변수를 통해 원하는 기간의 데이터를 다운로드 받을 수도 있습니다. from에는 시작 시점을, to에는 종료 시점을 입력하여 주면, 해당 시점의 데이터가 다운로드 받아짐이 확인됩니다. getSymbols() 함수를 통해 다운로드 받은 데이터는 자동으로 티커와 동일한 변수명에 저장됩니다. 만일 티커명이 아닌 원하는 변수명에 데이터를 저장하고 싶을 경우, auto.assign 인자를 FALSE로 설정해주면 다운로드 받은 데이터가 원하는 변수에 저장됩니다. data = getSymbols(&#39;AAPL&#39;, from = &#39;2000-01-01&#39;, to = &#39;2018-12-31&#39;, auto.assign = FALSE) head(data) ## AAPL.Open AAPL.High AAPL.Low AAPL.Close AAPL.Volume ## 2000-01-03 3.745536 4.017857 3.631696 3.997768 133949200 ## 2000-01-04 3.866071 3.950893 3.613839 3.660714 128094400 ## 2000-01-05 3.705357 3.948661 3.678571 3.714286 194580400 ## 2000-01-06 3.790179 3.821429 3.392857 3.392857 191993200 ## 2000-01-07 3.446429 3.607143 3.410714 3.553571 115183600 ## 2000-01-10 3.642857 3.651786 3.383929 3.491071 126266000 ## AAPL.Adjusted ## 2000-01-03 2.655498 ## 2000-01-04 2.431611 ## 2000-01-05 2.467196 ## 2000-01-06 2.253689 ## 2000-01-07 2.360442 ## 2000-01-10 2.318927 한번에 여러 종목의 주가를 다운로드 받을 수도 있습니다. 아래 예제와 같이 페이스북과 엔비디아의 티커인 FB와 NVDA를 ticker 변수에 입력하여 주고, getSymbols() 함수에 티커들을 입력한 변수를 넣어주면 두 종목의 주가가 동시에 다운로드 됩니다. ticker = c(&#39;FB&#39;, &#39;NVDA&#39;) getSymbols(ticker) ## [1] &quot;FB&quot; &quot;NVDA&quot; head(FB) ## FB.Open FB.High FB.Low FB.Close FB.Volume FB.Adjusted ## 2012-05-18 42.05 45.00 38.00 38.23 573576400 38.23 ## 2012-05-21 36.53 36.66 33.00 34.03 168192700 34.03 ## 2012-05-22 32.61 33.59 30.94 31.00 101786600 31.00 ## 2012-05-23 31.37 32.50 31.36 32.00 73600000 32.00 ## 2012-05-24 32.95 33.21 31.77 33.03 50237200 33.03 ## 2012-05-25 32.90 32.95 31.11 31.91 37149800 31.91 head(NVDA) ## NVDA.Open NVDA.High NVDA.Low NVDA.Close NVDA.Volume ## 2007-01-03 24.71333 25.01333 23.19333 24.05333 28870500 ## 2007-01-04 23.96667 24.05333 23.35333 23.94000 19932400 ## 2007-01-05 23.37333 23.46667 22.28000 22.44000 31083600 ## 2007-01-08 22.52000 23.04000 22.13333 22.60667 16431700 ## 2007-01-09 22.64000 22.79333 22.14000 22.16667 19104100 ## 2007-01-10 21.93333 23.46667 21.60000 23.26000 27718600 ## NVDA.Adjusted ## 2007-01-03 22.19124 ## 2007-01-04 22.08669 ## 2007-01-05 20.70281 ## 2007-01-08 20.85657 ## 2007-01-09 20.45063 ## 2007-01-10 21.45933 3.2.2 국내 종목 주가 다운로드 getSymbols() 함수를 이용하면 미국뿐 아니라 국내 종목의 주가를 다운로드 받을 수도 있습니다. 국내 종목의 티커는 총 6자리로 구성되어 있으며, 해당 함수에 입력되는 티커는 코스피 상장 종목의 경우 “티커.KS”, 코스닥 상장 종목의 경우 “티커.KQ”의 형태로 입력해 주어야 합니다. 먼저 코스피 상장종목인 삼성전자 데이터의 다운로드 예시입니다. getSymbols(&#39;005930.KS&#39;, from = &#39;2000-01-01&#39;, to = &#39;2018-12-31&#39;) ## [1] &quot;005930.KS&quot; tail(Ad(`005930.KS`)) ## 005930.KS.Adjusted ## 2018-12-20 38293.23 ## 2018-12-21 38293.23 ## 2018-12-24 38441.84 ## 2018-12-26 37996.00 ## 2018-12-27 38250.00 ## 2018-12-28 38700.00 삼성전자의 티커인 005930에 .KS를 붙여 함수에 입력할 경우, 티커명에 해당하는 005930.KS 변수명에 데이터가 저장됩니다. 변수명에 콤마(.)가 있는 관계로, Ad 함수를 통해 수정주가를 확인하고자 할 때는 변수명의 앞뒤에 억음부호(`)를 붙여주어야 합니다. 국내 종목의 경우 가끔 수정주가에 오류가 발생하는 경우가 많습니다. 배당이 반영된 값 보다는 단순 종가(Close) 데이터를 사용하기를 권장합니다. tail(Cl(`005930.KS`)) ## 005930.KS.Close ## 2018-12-20 38650 ## 2018-12-21 38650 ## 2018-12-24 38800 ## 2018-12-26 38350 ## 2018-12-27 38250 ## 2018-12-28 38700 Cl() 함수는 Close, 즉 종가만을 선택하여 주며, 사용 방법은 기존 Ad() 함수와 동일합니다. 비록 배당을 고려할 수는 없지만, 전반적으로 오류가 없는 데이터를 사용할 수 있습니다. 다음은 코스닥 상장종목인 셀트리온제약의 예시이며, 티커인 068670에 .KQ를 붙여 함수에 입력합니다. 역시나 데이터가 다운로드 되어 티커명의 변수에 저장됩니다. getSymbols(&quot;068760.KQ&quot;, from = &#39;2000-01-01&#39;, to = &#39;2018-12-31&#39;) ## [1] &quot;068760.KQ&quot; tail(Cl(`068760.KQ`)) ## 068760.KQ.Close ## 2018-01-24 95100 ## 2018-01-25 97300 ## 2018-01-26 97400 ## 2018-01-29 99900 ## 2018-01-30 99500 ## 2018-01-31 97500 3.2.3 FRED 데이터 다운로드 미국 및 각국의 중요 경제지표 데이터를 살펴볼 때, 가장 많이 참조되는 곳 중 하나가 미 연방 준비 은행에서 관리하는 Fred Economic Date 입니다. getSymbols() 함수를 통해 FRED 데이터를 다운로드 받을 수 있습니다. 먼저 미 국채 10년물 금리를 다운로드 받는 예제를 살펴보도록 하겠습니다. getSymbols(&#39;DGS10&#39;, src=&#39;FRED&#39;) ## [1] &quot;DGS10&quot; chart_Series(DGS10) 먼저 미 국채 10년물 금리에 해당하는 티커인 “DGS10”을 입력해 줍니다. 그 후, 데이터 소스에 해당하는 src에 “FRED”를 입력해 주면, 해당 데이터가 다운로드 됩니다. chart_Series를 통해 해당 데이터를 그래프로 나타내면 다음과 같습니다. 각 항목 별 티커를 찾는 방법은 매우 간단합니다. 먼저 FRED의 홈페이지9에 접속하여, 원하는 데이터를 검색합니다. 예시로써 원/달러 환율에 해당하는 South Korea / U.S. Foreign Exchange Rate 를 검색하여 원하는 페이지에 접속합니다. 이 중 페이지 주소에서 /series/ 다음에 위치하는 DEXKOUS 가 해당 항목의 티커입니다. Figure 3.1: FRED 사이트 내 원/달러 환율의 티커 확인 해당 티커를 입력하면, 홈페이지와 동일한 데이터가 다운로드 됨이 확인됩니다. 이 외에도 509,000 여개의 방대한 FRED 데이터를 해당 함수를 통해 손쉽게 R에서 다운로드 받을 수 있습니다. getSymbols(&#39;DEXKOUS&#39;, src=&#39;FRED&#39;) ## [1] &quot;DEXKOUS&quot; tail(DEXKOUS) ## DEXKOUS ## 2019-05-31 1190.50 ## 2019-06-03 1180.97 ## 2019-06-04 1180.58 ## 2019-06-05 1178.24 ## 2019-06-06 1179.51 ## 2019-06-07 1181.27 chart_Series(DEXKOUS) 자세한 내용은 https://docs.quandl.com/ 에서 확인할 수 있습니다.↩ https://fred.stlouisfed.org/↩ "],
["section-12.html", "Chapter 4 크롤링 이해하기 4.1 GET과 POST 방식 이해하기 4.2 크롤링 예제", " Chapter 4 크롤링 이해하기 앞선 장에서 볼 수 있듯이 API를 이용할 경우 데이터를 매우 쉽게 수집할 수 있지만, 국내 주식 데이터를 다운로드 받기에는 한계가 있으며, 우리가 원하는 데이터가 API의 형태로 제공된다는 보장도 없습니다. 따라서 우리는 필요한 데이터를 얻기 위해 직접 찾아나서야 합니다. 다행히도 금융 사이트들에는 주가, 재무정보 등 우리가 원하는 대부분의 주식 정보가 제공되고 있으며, API를 활용할 수 없는 경우에도 크롤링을 통해 이러한 데이터를 수집할 수 있습니다. 크롤링 혹은 스크래핑이란 웹사이트에서 원하는 정보를 수집하는 기술입니다. 대부분의 금융 사이트들이 간단한 형태로 작성되어 있어, 몇 가지 기술만 익히면 어렵지 않게 데이터를 크롤링 할 수 있습니다. 그러나 크롤링에 대한 대부분의 강의나 설명이 파이썬을 이용한 방법으로써, 초보자가 R을 이용한 크롤링을 배우는데는 어려움이 있습니다. 해당 장에서는 크롤링에 대한 간단한 설명과 예제를 살펴보도록 하겠습니다. 크롤링을 할 때는 주의해야 할 점이 있습니다. 특정 사이트의 페이지를 쉬지 않고 크롤링을 하는 행위를 무한 크롤링이라 합니다. 이러한 경우 해당 사이트의 자원을 독점하게 되어 타인의 사용을 막게 되며, 사이트에 부하를 주게 됩니다. 일부 사이트에서는 동일한 IP로 쉬지 않고 크롤링을 할 경우 접속을 막아버리는 경우도 있습니다. 따라서 하나의 페이지를 크롤링 한 후, 1~2초 가량 정지한 후 다시 다음 페이지를 크롤링 할 필요가 있습니다. 4.1 GET과 POST 방식 이해하기 우리가 인터넷에 접속하여 서버에 파일을 요청하면, 서버는 이에 해당하는 파일을 우리에게 보내줍니다. 이러한 과정을 사람이 수행하기 편하고 시각적으로 보기 편하도록 만들어 진 것이 크롬과 같은 웹브라우저 이며, 서버의 주소를 기억하기 쉽게하기 위해 만든 것이 인터넷 주소 입니다. 우리가 서버에 데이터를 요청하는 형태는 다양하지만 크롤링에서는 주로 GET과 POST 방식을 사용합니다. Figure 1.1: 클라이언트와 서버 간의 요청/응답 과정 4.1.1 GET 방식 GET 방식은 인터넷 주소를 기준으로, 이에 해당하는 데이터나 파일을 요청하는 것입니다. 주로 클라이언트가 요청하는 쿼리를 앰퍼샌드(&amp;) 혹은 물음표(?) 형식으로 결합하여 서버에 전달됩니다. 한경컨센서스10에 접속한 후 전체 REPORT를 선택하면, 홈페이지의 주소 뒤에 /apps.analysis/analysis.list가 붙으며 이에 해당하는 페이지의 내용을 보여줍니다. 상단의 탭에서 기업을 선택하면, 주소의 끝부분에 ?skinType=business가 추가되며 이에 해당하는 페이지의 내용을 보여줍니다. 즉, 해당 페이지는 GET 방식을 사용하고 있으며 입력종류는 skinType, 이에 해당하는 기업 탭의 입력값은 business 임을 알 수 있습니다. Figure 1.2: 한경 컨센서스 기업 REPORT 페이지 이번에는 파생 탭을 선택하여 봅니다. 역시나 홈페이지 주소가 변경되며 해당 주소에 맞는 내용이 나타납니다. 주소의 끝부분이 ?skinType=derivative 로 변경되며, 입력 값이 변경됨에 따라 페이지의 내용이 이에 맞게 변하는 모습이 확인됩니다. 여러 다른 탭들을 눌러보면 ?skinType= 뒷부분의 입력값이 변함에 따라 이에 해당하는 페이지로 내용이 변경됨이 확인됩니다. 다시 기업 탭을 선택한 후, 다음 페이지를 확인하기 위해 하단의 2를 클릭합니다. 기존 주소인 ?skinType=business 뒤에 추가로 sdate와 edate, 그리고 now_page 쿼리가 추가됨이 확인됩니다. sdate에 검색 기간의 시작시점, edate에 검색 기간의 종료시점, now_page에 원하는 페이지를 수기로 입력해도 이에 해당하는 페이지의 데이터를 보여줍니다. 이처럼 GET 방식으로 데이터를 요청할 경우, 웹 페이지 주소를 수정하여 원하는 종류의 데이터를 받아올 수 있습니다. Figure 1.3: 쿼리 추가로 인한 url의 변경 4.1.2 POST 방식 POST 방식은 사용자가 필요한 값을 추가해서 요청하는 방법입니다. GET 방식과의 차이는 클라이언트가 요청하는 쿼리를 body에 넣어서 전송하므로, 요청 내역을 직접적으로 볼 수 없습니다. 한국거래소 상장공시시스템11에 접속하여 전체메뉴보기를 누른 후, 상장법인상세정보 중 상장종목현황을 선택합니다. 웹 페이지 주소가 바뀌며, 상장종목현황이 보여집니다. Figure 1.4: 상장공시시스템의 상장종목현황 메뉴 이번엔 조회일자를 2017-12-28로 선택한 후, 검색을 눌러보도록 합니다. 페이지의 내용은 선택일 기준으로 변경되었지만, 주소는 변경되지 않고 그대로 남아있습니다. GET 방식에서는 선택항목에 따라 웹 페이지 주소가 변경되었지만, POST 방식을 사용하여 서버에 데이터를 요청하는 해당 사이트는 그렇지 않음이 확인됩니다. POST 방식의 데이터 요청과정을 살펴보기 위해서는 개발자도구 이용해야 하며, 크롬 브라우저에서 F12 키를 눌러 해당 화면을 열 수 있습니다. 개발자도구 화면을 연 상태에서 다시 한번 ’검색’을 클릭해 봅니다. Network 탭을 클릭하면, ’검색’을 클릭함과 함게 브라우저와 서버간의 통신 과정을 살펴볼 수 있습니다. 이 중 listedIssueStatus.do 라는 항목이 POST 형태임을 알 수 있습니다. Figure 2.1: 크롬 개발자도구의 Network 화면 해당 메뉴를 클릭하면 통신 과정을 좀 더 자세히 알 수 있습니다. 가장 하단의 Form Data에 서버에 데이터를 요청하는 내역이 있습니다. method에는 readListIssueStatus, selDate에는 2017-12-28라는 값이 있습니다. Figure 4.1: POST 방식의 서버 요청 내역 이처럼 POST 방식은 요청하는 데이터에 대한 쿼리가 GET 방식처럼 url을 통해 전송되는 것이 아닌 body를 통해 전송되므로, 이에 대한 정보는 웹브라우저를 통해 확인할 수는 없습니다. 4.2 크롤링 예제 크롤링의 일반적인 과정은 httr 패키지의 GET() 혹은 POST() 함수를 이용하여 데이터를 다운로드 받은 후, rvest 패키지의 함수들을 이용하여 원하는 데이터를 찾아내는 과정으로 이루어집니다. 해당 장에서는 GET 방식의 예제로 금융 실시간 속보의 제목을 추출하는 방법을, POST 방식의 예제로 기업공시채널에서 오늘의 공시를 추출하는 방법을, 마지막으로 태그와 속성, 페이지 네비게이션 값을 결합하여 국내 상장 주식의 종목명 및 티커를 추출하는 방법에 대해 알아보도록 하겠습니다. 4.2.1 금융 속보 크롤링 크롤링의 간단한 예제로 금융 속보의 제목을 추출해 보도록 하겠습니다. 먼저 네이버 금융에 접속한 후 뉴스 → 실시간 속보12를 선택해 줍니다. 이 중 뉴스의 제목에 해당하는 텍스트만 추출하고자 합니다. 뉴스 제목 부분에 마우스를 올려둔 후 우클릭 → 검사를 선택할 경우 개발자도구 화면이 열리며, 해당 글자가 html 내에서 어떤 부분에 위치하는지 확인할 수 있습니다. 해당 제목은 dl 태그 → dd 태그의 articleSubject 클래스 → a 태그 중 title 속성에 위치하고 있습니다. 태그와 속성의 차이가 이해되지 않으시는 분은 태그와 속성 부분을 다시 살펴보시기 바랍니다. Figure 4.2: 실시간 속보의 제목 부분 html 먼저 해당 페이지의 내용을 R로 불러오도록 하겠습니다. library(rvest) library(httr) url = &#39;https://finance.naver.com/news/news_list.nhn?mode=LSS2D&amp;section_id=101&amp;section_id2=258&#39; data = GET(url) print(data) ## Response [https://finance.naver.com/news/news_list.nhn?mode=LSS2D&amp;section_id=101&amp;section_id2=258] ## Date: 2019-06-13 13:28 ## Status: 200 ## Content-Type: text/html;charset=EUC-KR ## Size: 55.5 kB ## ## ## ## ## ## ## ## ## &lt;!-- global include --&gt; ## ## ... 먼저 url 변수에 해당 주소를 입력한 후, GET() 함수를 이용하여 해당 페이지의 내용을 받아 data 변수에 저장합니다. data 변수를 확인해보면 Status가 200, 즉 데이터가 이상없이 받아졌으며, 인코딩은 EUC-KR 타입으로 되어 있습니다. 우리는 개발자도구 화면을 통해 제목에 해당하는 부분이 dl 태그 → dd 태그의 articleSubject 클래스 → a 태그 중 title 속성에 위치하고 있음을 살펴보았습니다. 이를 활용해 제목 부분만을 추출하는 방법은 다음과 같습니다. data_title = data %&gt;% read_html(encoding = &#39;EUC-KR&#39;) %&gt;% html_nodes(&#39;dl&#39;) %&gt;% html_nodes(&#39;.articleSubject&#39;) %&gt;% html_nodes(&#39;a&#39;) %&gt;% html_attr(&#39;title&#39;) 먼저 read_html() 함수를 이용하여 해당 페이지의 html 내용을 읽어오며, 인코딩은 ’EUC-KR’로 셋팅해주도록 합니다. html_nodes() 함수는 해당 태그를 추출하는 함수로써, dl 태그에 해당하는 부분을 추출합니다. html_nodes() 함수를 이용하여 articleSubject 클래스에 해당하는 부분을 추출할 수 있으며, 클래스 속성의 경우 이름 앞에 콤마(.)를 붙여주어야 합니다. html_nodes() 함수를 이용하여 a 태그를 추출합니다. html_attr은 속성을 추출하는 함수로써, title에 해당하는 부분만을 추출합니다. 해당 과정을 거쳐 data_title에는 실시간 속보의 제목만이 저장되게 됩니다. 이처럼 개발자도구 화면을 통해 내가 추출하고자 하는 데이터가 html 중 어디에 위치하고 있는지 먼저 확인을 하면, 어렵지 않게 원하는 데이터를 읽어올 수 있습니다. print(data_title) ## [1] &quot;성동조선해양 3차 매각도 물거품&quot; ## [2] &quot;&#39;테러 드론&#39; 잡는 &#39;5G 가드 드론&#39;&quot; ## [3] &quot;&#39;비혼&#39; 인구 는다…\\&quot;결혼하지 않는 것도 선택\\&quot;&quot; ## [4] &quot;\\&quot;대기업 제휴돼 있다\\&quot;…맘카페 통해 번지는 &#39;가전제품 사기&#39;&quot; ## [5] &quot;투자? 상거래?…정부는 &#39;떠넘기기&#39;&quot; ## [6] &quot;10만 원 투자하면 고급 이어폰 준다더니…&quot; ## [7] &quot;13일 장 마감 후 주요 종목 뉴스&quot; ## [8] &quot;거래소 &quot; ## [9] &quot;[유가증권시장 공시] KCC / 동부건설&quot; ## [10] &quot;[표]아시아 주요 증시 동향(6월 13일)&quot; ## [11] &quot;코오롱티슈진, 93억원 손해배상소송 피소&quot; ## [12] &quot;에이치엘비, 1879억원 규모 3자배정 유상증자 결정&quot; ## [13] &quot;오늘의 재운[6월 14일]&quot; ## [14] &quot;재계 의결권 자문 기구 &#39;지배구조위원회&#39; 20일 출범&quot; ## [15] &quot;EMW, 상장폐지 개선계획 이행내역서 제출&quot; ## [16] &quot;UCI, 전환사채 80억원치 발행&quot; ## [17] &quot;코디, 내년 4월13일까지 상장폐지 개선기간&quot; ## [18] &quot;장 마감 후 기업공시[6월 13일]&quot; ## [19] &quot;코다코, 車부품업체 지코 지분 45만주 처분&quot; ## [20] &quot;데일리블록체인, 4억원 규모 3자배정 유상증자&quot; 4.2.2 기업공시채널에서 오늘의 공시 불러오기 한국거래소 상장공시시스템에 접속한 후 오늘의 공시 → 전체 → 더보기를 선택하여 전체 공시내용을 확인할 수 있습니다. Figure 4.3: 오늘의공시 확인하기 해당 페이지에서 날짜를 변경할 경우, 페이지의 내용은 해당일의 공시로 변경되지만 url은 변경되지 않습니다. 이처럼 POST 방식의 경우 요청하는 데이터에 대한 쿼리가 body의 형태를 통해 전송되므로, 개발자도구 화면을 통해 해당 쿼리에 대한 내용을 확인할 수 있습니다. 개발자도구 화면을 연 상태에서 조회일자를 2018-12-28로 선택한 후 Network 탭의 todaydisclosure.do 항목을 살펴보면 Form Data를 통해 서버에 데이터를 요청하는 내역을 확인할 수 있습니다. 여러 항목 중 selDate 부분이 우리가 선택한 일자로 설정되어 있습니다. Figure 2.4: POST 방식의 데이터 요청 POST 방식으로 쿼리를 요청하는 방법을 코드로 나타내면 다음과 같습니다. library(httr) library(rvest) Sys.setlocale(&quot;LC_ALL&quot;, &quot;English&quot;) ## [1] &quot;LC_COLLATE=English_United States.1252;LC_CTYPE=English_United States.1252;LC_MONETARY=English_United States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252&quot; url = &#39;http://kind.krx.co.kr/disclosure/todaydisclosure.do&#39; data = POST(url, body = list( method = &#39;searchTodayDisclosureSub&#39;, currentPageSize = &#39;15&#39;, pageIndex = &#39;1&#39;, orderMode = &#39;0&#39;, orderStat = &#39;D&#39;, forward = &#39;todaydisclosure_sub&#39;, chose = &#39;S&#39;, todayFlag = &#39;Y&#39;, selDate = &#39;2018-12-28&#39; )) data = read_html(data) %&gt;% html_table(fill = TRUE) %&gt;% .[[1]] Sys.setlocale(&quot;LC_ALL&quot;, &quot;Korean&quot;) ## [1] &quot;LC_COLLATE=Korean_Korea.949;LC_CTYPE=Korean_Korea.949;LC_MONETARY=Korean_Korea.949;LC_NUMERIC=C;LC_TIME=Korean_Korea.949&quot; 한글로 작성된 페이지를 크롤링 할 경우 오류가 발생하는 경우가 종종 있으므로, Sys.setlocale() 함수를 통해 로케일 언어를 영어로 설정 해줍니다. POST() 함수를 통해 해당 url에 원하는 쿼리를 요청해주며, 쿼리는 body 내에 list 형태로 입력해주도록 합니다. 해당 값은 개발자도구 화면의 Form Data와 동일하게 입력해주며, marketType과 같이 값이 존재하지 않는 항목은 입력하지 않아도 됩니다. read_html() 함수를 이용하여 해당 페이지의 html 내용을 읽어옵니다. html_table() 함수는 테이블 형태의 데이터를 읽어오는 함수입니다. 셀 간 병합이 된 열이 존재하므로 fill=TRUE 를 추가해주도록 합니다. .[[1]]를 통해 첫번째 리스트를 선택해 줍니다. 한글을 읽기 위해 Sys.setlocale() 함수를 통해 로케일 언어를 다시 Korean으로 변경해 줍니다. 저장된 데이터를 확인하면 화면과 동일한 내용이 출력됩니다. print(head(data)) ## NA NA NA ## 1 18:32 화신테크 최대주주변경 ## 2 18:26 에스제이케이 증권 발행결과(자율공시)(제3자배정 유상증자) ## 3 18:11 아이엠텍 [정정]유상증자결정(제3자배정) ## 4 18:10 시그넷이브이 유형자산 양수 결정 ## 5 18:09 자기주식매매신청내역(코스닥시장) ## 6 18:09 대량매매내역(코스닥시장) ## NA NA ## 1 화신테크 공시차트\\r\\n\\t\\t\\t\\t\\t주가차트 ## 2 에스제이케이 공시차트\\r\\n\\t\\t\\t\\t\\t주가차트 ## 3 아이엠텍 공시차트\\r\\n\\t\\t\\t\\t\\t주가차트 ## 4 시그넷이브이 공시차트\\r\\n\\t\\t\\t\\t\\t주가차트 ## 5 코스닥시장본부 ## 6 코스닥시장본부 POST 형식의 경우 body에 들어가는 쿼리 내용을 바꾸어 원하는 데이터를 받을수 있습니다. 만일 2019년 1월 4일 공시를 확인하고자 할 경우, 위의 코드에서 selDate만 ’2019-01-04’로 변경해주면 됩니다. 아래 코드의 출력 결과물을 2019년 1월 4일 공시와 확인하면 동일한 결과임을 확인할 수 있습니다. Sys.setlocale(&quot;LC_ALL&quot;, &quot;English&quot;) ## [1] &quot;LC_COLLATE=English_United States.1252;LC_CTYPE=English_United States.1252;LC_MONETARY=English_United States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252&quot; url = &#39;http://kind.krx.co.kr/disclosure/todaydisclosure.do&#39; data = POST(url, body = list( method = &#39;searchTodayDisclosureSub&#39;, currentPageSize = &#39;15&#39;, pageIndex = &#39;1&#39;, orderMode = &#39;0&#39;, orderStat = &#39;D&#39;, forward = &#39;todaydisclosure_sub&#39;, chose = &#39;S&#39;, todayFlag = &#39;Y&#39;, selDate = &#39;2019-01-04&#39; )) data = read_html(data) %&gt;% html_table(fill = TRUE) %&gt;% .[[1]] Sys.setlocale(&quot;LC_ALL&quot;, &quot;Korean&quot;) ## [1] &quot;LC_COLLATE=Korean_Korea.949;LC_CTYPE=Korean_Korea.949;LC_MONETARY=Korean_Korea.949;LC_NUMERIC=C;LC_TIME=Korean_Korea.949&quot; print(head(data)) ## NA NA ## 1 18:18 휴벡셀 ## 2 18:15 케이엠더블유 ## 3 18:15 스튜디오썸머 ## 4 18:14 스튜디오썸머 ## 5 18:10 헬릭스미스 ## 6 18:10 KJ프리텍 ## NA ## 1 [정정]최대주주 변경 ## 2 불성실공시법인지정예고(공시변경) ## 3 [정정]최대주주 변경을 수반하는 주식 담보제공 계약 해제ㆍ취소 등 ## 4 [정정]최대주주 변경을 수반하는 주식 담보제공 계약 체결 ## 5 공매도 과열종목 지정(공매도 거래 금지 적용) ## 6 공매도 과열종목 지정(공매도 거래 금지 적용) ## NA NA ## 1 휴벡셀 공시차트\\r\\n\\t\\t\\t\\t\\t주가차트 ## 2 코스닥시장본부 공시차트\\r\\n\\t\\t\\t\\t\\t주가차트 ## 3 스튜디오 썸머 공시차트\\r\\n\\t\\t\\t\\t\\t주가차트 ## 4 스튜디오 썸머 공시차트\\r\\n\\t\\t\\t\\t\\t주가차트 ## 5 코스닥시장본부 공시차트\\r\\n\\t\\t\\t\\t\\t주가차트 ## 6 코스닥시장본부 공시차트\\r\\n\\t\\t\\t\\t\\t주가차트 4.2.3 네이버 금융에서 주식티커 크롤링 태그와 속성, 페이지 네비게이션 값을 결합하여 국내 상장 주식의 종목명 및 티커를 추출하는 방법에 대해 알아보도록 하겠습니다. 네이버 금융에서 국내증시 → 시가총액 페이지에는 코스피와 코스닥의 시가총액별 정보가 나타나 있습니다. 코스피: https://finance.naver.com/sise/sise_market_sum.nhn?sosok=0&amp;page=1 코스닥: https://finance.naver.com/sise/sise_market_sum.nhn?sosok=1&amp;page=1 또한 각 종목명을 클릭하여 이동하는 페이지의 url을 확인해보면, 끝 6자리가 각 종목의 거래소 티커임도 확인이 됩니다. 티커 정리를 위해 우리가 html에서 확인해야 할 부분은 총 2가지 입니다. 먼저 하단의 페이지 네비게이션을 통해 코스피와 코스닥 시가총액에 해당하는 페이지가 각각 몇번째 페이지까지 존재하는지를 알아야 합니다. 아래와 같은 항목 중 맨뒤에 해당하는 페이지가 가장 마지막 페이지에 해당합니다. Figure 2.6: 페이지 네비게이션 맨뒤 글자에 마우스를 올려둔 후 우클릭 → 검사를 선택할 경우 개발자도구 화면이 열리며, 해당 글자가 html 내에서 어떤 부분에 위치하는지 확인할 수 있습니다. ‘맨뒤’ 에 해당하는 링크는 pgRR 클래스 → a 태그 중 href 속성에 위치하며, page= 뒷부분의 숫자에 위치하는 페이지로 링크가 걸려있습니다. Figure 4.4: HTML 내 페이지 네비게이션 부분 종목명 링크에 해당하는 주소 중 끝 6자리는 티커에 해당합니다. 따라서 각 링크들의 주소를 알아야 할 필요도 있습니다. Figure 2.7: 네이버 금융 시가총액 페이지 삼성전자에 마우스를 올려둔 후 우클릭 → 검사를 통해 개발자도구 화면을 살펴보면, 해당 링크가 tbody → td → a 태그에서 href 속성에 위치하고 있음을 알수 있습니다. 위의 정보들을 이용하여 데이터를 다운로드 받도록 하겠습니다. 아래 코드에서 i = 0 일 경우 코스피에 해당하는 url이, i = 1 일 경우 코스닥에 해당하는 url이 생성되며, 먼저 코스피에 해당하는 데이터를 다운로드 받도록 하겠습니다. library(httr) library(rvest) i = 0 ticker = list() url = paste0(&#39;https://finance.naver.com/sise/sise_market_sum.nhn?sosok=&#39;,i,&#39;&amp;page=1&#39;) print(url) ## [1] &quot;https://finance.naver.com/sise/sise_market_sum.nhn?sosok=0&amp;page=1&quot; down_table = GET(url) print(down_table) ## Response [https://finance.naver.com/sise/sise_market_sum.nhn?sosok=0&amp;page=1] ## Date: 2019-06-13 13:28 ## Status: 200 ## Content-Type: text/html;charset=EUC-KR ## Size: 95.1 kB ## ## ## ## ## ## ## ## &lt;!-- global include --&gt; ## ## ## ... 빈 리스트인 ticker 변수를 만들어 줍니다. paste0() 함수를 이용하여 코스피 시가총액 페이지의 url을 만듭니다. GET() 함수를 통해 해당 페이지 내용을 받아 down_table 변수에 저장합니다. down_table 변수를 확인해보면 Status가 200, 즉 데이터가 이상없이 받아졌으며, 인코딩은 EUC-KR 타입으로 되어 있습니다. 가장 먼저 해야할 작업은 가장 마지막 페이지가 몇번째 페이지인지 찾아내는 작업입니다. 우리는 이미 개발자도구 화면을 통해 해당 정보가 pgRR 클래스의 a태그 중 href 속성에 위치하고 있음을 알고 있습니다. navi.final = read_html(down_table, encoding = &#39;EUC-KR&#39;) %&gt;% html_nodes(., &#39;.pgRR&#39;) %&gt;% html_nodes(., &#39;a&#39;) %&gt;% html_attr(., &#39;href&#39;) read_html() 함수를 이용하여 해당 페이지의 html 내용을 읽어오며, 인코딩은 ’EUC-KR’로 셋팅해주도록 합니다. html_nodes() 함수를 이용하여 pgRR 클래스 정보만을 불러오도록 하며, 클래스 속성이므로 앞에 콤마(.)를 붙여 주도록 합니다. html_nodes() 함수를 통해 a 태그 정보만을 불러오도록 합니다. html_attr() 함수를 통해 href 속성을 불러오도록 합니다. 이를 통해 navi.final에는 해당 부분에 해당하는 내용이 저장됩니다. print(navi.final) ## [1] &quot;/sise/sise_market_sum.nhn?sosok=0&amp;page=31&quot; 이 중 우리가 알고싶은 내용은 page= 뒤에 존재하는 숫자입니다. 해당 내용을 추출하는 코드는 다음과 같습니다. navi.final = navi.final %&gt;% strsplit(., &#39;=&#39;) %&gt;% unlist() %&gt;% tail(., 1) %&gt;% as.numeric() strsplit() 함수는 전체 문장을 특정 글자 기준으로 나누는 것입니다. page= 뒷부분만의 데이터가 필요하므로 ’=’를 기준으로 문장을 나눠주도록 합니다. unlist() 함수를 통해 결과를 벡터 형태로 변환합니다. tail() 함수를 통해 마지막 첫번째 데이터만 선택합니다. as.numeric() 함수를 통해 해당 값을 숫자 형태로 바꾸어 주도록 합니다. print(navi.final) ## [1] 31 for loop 구문을 이용할 경우 1 페이지 부터 navi.final, 즉 마지막 페이지까지 모든 페이지의 내용을 읽어올 수 있습니다. 먼저 코스피의 첫번째 페이지에서 우리가 원하는 데이터를 추출하는 방법을 살펴보도록 하겠습니다. i = 0 # 코스피 j = 1 # 첫번째 페이지 url = paste0(&quot;https://finance.naver.com/sise/sise_market_sum.nhn?sosok=&quot;,i,&quot;&amp;page=&quot;,j) down_table = GET(url) i와 j에 각각 0과 1을 입력하여 코스피 첫번째 페이지에 해당하는 url을 생성해 줍니다. GET() 함수를 이용하여 해당 페이지의 데이터를 다운로드 받습니다. Sys.setlocale(&quot;LC_ALL&quot;, &quot;English&quot;) ## [1] &quot;LC_COLLATE=English_United States.1252;LC_CTYPE=English_United States.1252;LC_MONETARY=English_United States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252&quot; table = read_html(down_table, encoding = &quot;EUC-KR&quot;) %&gt;% html_table(fill = TRUE) table = table[[2]] Sys.setlocale(&quot;LC_ALL&quot;, &quot;Korean&quot;) ## [1] &quot;LC_COLLATE=Korean_Korea.949;LC_CTYPE=Korean_Korea.949;LC_MONETARY=Korean_Korea.949;LC_NUMERIC=C;LC_TIME=Korean_Korea.949&quot; Sys.setlocale() 함수를 통해 로케일 언어를 영어로 설정 해줍니다. read_html() 함수를 통해 html 정보를 읽어옵니다. html_table() 함수를 통해 테이블 정보를 읽어오며, fill=TRUE 를 추가해줍니다. table 변수에는 리스트 형태로 총 3가지 테이블이 저장되어 있습니다. 첫번째 리스트에는 거래량, 시가, 고가 등 적용 항목, 세번째 리스트에는 페이지 네비게이션 테이블이 저장되어 있으므로, 우리에게 필요한 두번째 리스트만을 table 변수에 다시 저장하도록 합니다. 한글을 읽기 위해 Sys.setlocale() 함수를 통해 로케일 언어를 다시 Korean으로 변경해 줍니다. 저장된 table 내용을 확인하면 다음과 같습니다. print(head(table)) ## N 종목명 현재가 전일비 등락률 액면가 시가총액 상장주식수 ## 1 NA ## 2 1 삼성전자 43,750 850 -1.91% 100 2,611,780 5,969,783 ## 3 2 SK하이닉스 63,500 2,200 -3.35% 5,000 462,282 728,002 ## 4 3 현대차 141,000 0 0.00% 5,000 301,272 213,668 ## 5 4 삼성전자우 36,200 550 -1.50% 100 297,885 822,887 ## 6 5 셀트리온 207,000 3,000 +1.47% 1,000 265,641 128,329 ## 외국인비율 거래량 PER ROE 토론실 ## 1 NA &lt;NA&gt; &lt;NA&gt; NA ## 2 57.10 16,889,337 7.26 19.63 NA ## 3 50.23 5,907,888 2.97 38.53 NA ## 4 44.59 906,125 26.35 2.20 NA ## 5 92.52 1,988,327 6.01 N/A NA ## 6 20.46 506,730 101.02 10.84 NA 이 중 마지막 열인 토론실은 필요가 없는 열이며, 첫번째 행과 같이 아무런 정보가 없는 행이 존재하기도 합니다. 이를 정리하면 다음과 같습니다. table[, ncol(table)] = NULL table = na.omit(table) print(head(table)) ## N 종목명 현재가 전일비 등락률 액면가 시가총액 상장주식수 ## 2 1 삼성전자 43,750 850 -1.91% 100 2,611,780 5,969,783 ## 3 2 SK하이닉스 63,500 2,200 -3.35% 5,000 462,282 728,002 ## 4 3 현대차 141,000 0 0.00% 5,000 301,272 213,668 ## 5 4 삼성전자우 36,200 550 -1.50% 100 297,885 822,887 ## 6 5 셀트리온 207,000 3,000 +1.47% 1,000 265,641 128,329 ## 10 6 LG화학 349,000 12,000 +3.56% 5,000 246,367 70,592 ## 외국인비율 거래량 PER ROE ## 2 57.10 16,889,337 7.26 19.63 ## 3 50.23 5,907,888 2.97 38.53 ## 4 44.59 906,125 26.35 2.20 ## 5 92.52 1,988,327 6.01 N/A ## 6 20.46 506,730 101.02 10.84 ## 10 38.51 335,836 18.55 8.86 이제 우리가 필요한 정보는 6자리 티커입니다. 티커 역시 개발자도구 화면을 통해 tbody → td → a 태그에서 href 속성에 위치하고 있음을 알고 있으며, 이를 추출하는 코드는 다음과 같습니다. symbol = read_html(down_table, encoding = &#39;EUC-KR&#39;) %&gt;% html_nodes(., &#39;tbody&#39;) %&gt;% html_nodes(., &#39;td&#39;) %&gt;% html_nodes(., &#39;a&#39;) %&gt;% html_attr(., &#39;href&#39;) print(head(symbol, 10)) ## [1] &quot;/item/main.nhn?code=005930&quot; &quot;/item/board.nhn?code=005930&quot; ## [3] &quot;/item/main.nhn?code=000660&quot; &quot;/item/board.nhn?code=000660&quot; ## [5] &quot;/item/main.nhn?code=005380&quot; &quot;/item/board.nhn?code=005380&quot; ## [7] &quot;/item/main.nhn?code=005935&quot; &quot;/item/board.nhn?code=005935&quot; ## [9] &quot;/item/main.nhn?code=068270&quot; &quot;/item/board.nhn?code=068270&quot; read_html()함수를 통해 html 정보를 읽어오며, 인코딩은 ’EUC-KR’로 설정합니다. html_nodes() 함수를 통해 ‘tbody’ 태그 정보를 불러옵니다. 다시 html_nodes() 함수를 통해 ‘td’와 ’a’ 태그 정보를 불러옵니다. html_attr() 함수를 이용하여 ‘href’ 속성을 불러옵니다. 이를 통해 symbol에는 href 속성에 해당하는 링크 주소들이 저장되게 됩니다. 이 중 마지막 6자리 글자만 추출하는 코드는 다음과 같습니다. symbol = sapply(symbol, function(x) { substr(x, nchar(x) - 5, nchar(x)) }) print(head(symbol, 10)) ## /item/main.nhn?code=005930 /item/board.nhn?code=005930 ## &quot;005930&quot; &quot;005930&quot; ## /item/main.nhn?code=000660 /item/board.nhn?code=000660 ## &quot;000660&quot; &quot;000660&quot; ## /item/main.nhn?code=005380 /item/board.nhn?code=005380 ## &quot;005380&quot; &quot;005380&quot; ## /item/main.nhn?code=005935 /item/board.nhn?code=005935 ## &quot;005935&quot; &quot;005935&quot; ## /item/main.nhn?code=068270 /item/board.nhn?code=068270 ## &quot;068270&quot; &quot;068270&quot; sapply() 함수를 통해 symbol 변수의 내용들에 function()을 적용하며, substr() 함수 내에 nchar() 함수를 적용하여 마지막 6자리 글자만을 추출하도록 합니다. 결과를 살펴보면 티커에 해당하는 마지막 6글자만 추출된 것이 확인됩니다. 그러나 결과를 살펴보면 동일한 내용이 두번 연속하여 추출됩니다. 이는 main.nhn?code= 에 해당하는 부분은 종목명에 설정된 링크, board.nhn?code= 에 해당하는 부분은 토론실에 설정된 링크이기 때문입니다. symbol = unique(symbol) print(head(symbol, 10)) ## [1] &quot;005930&quot; &quot;000660&quot; &quot;005380&quot; &quot;005935&quot; &quot;068270&quot; &quot;051910&quot; &quot;055550&quot; ## [8] &quot;017670&quot; &quot;051900&quot; &quot;207940&quot; unique() 함수를 이용하여 중복되는 티커를 제거하면 우리가 원하는 티커 부분만 깔끔하게 정리가 됩니다. 해당 내용을 위에서 구한 table에 입력한 후 데이터를 다듬는 과정은 다음과 같습니다. table$N = symbol colnames(table)[1] = &#39;종목코드&#39; rownames(table) = NULL ticker[[j]] = table ’N’열에 위에서 구한 티커를 입력해 줍니다. 해당 열 이름을 ’종목코드’로 변경합니다. na.omit()을 통해 특정 행을 삭제하였으므로, 행 이름을 초기화 해주도록 합니다. ticker의 j번째 리스트에 정리된 데이터를 입력해 줍니다. 위의 코드에서 i와 j값을 for loop를 이용하면 코스피와 코스닥 전 종목의 티커가 정리된 테이블을 만들 수 있습니다. 이를 전체 코드로 나타내면 다음과 같습니다. data = list() # i = 0 은 코스피, i = 1 은 코스닥 종목 for (i in 0:1) { ticker = list() url = paste0(&quot;https://finance.naver.com/sise/sise_market_sum.nhn?sosok=&quot;,i,&quot;&amp;page=1&quot;) down_table = GET(url) # 최종 페이지 번호 찾아주기 navi.final = read_html(down_table, encoding = &quot;EUC-KR&quot;) %&gt;% html_nodes(., &quot;.pgRR&quot;) %&gt;% html_nodes(., &quot;a&quot;) %&gt;% html_attr(.,&quot;href&quot;) %&gt;% strsplit(., &quot;=&quot;) %&gt;% unlist() %&gt;% tail(., 1) %&gt;% as.numeric() # 첫번째 부터 마지막 페이지까지 for loop를 이용하여 테이블 추출하기 for (j in 1:navi.final) { # 각 페이지에 해당하는 url 생성 url = paste0(&quot;https://finance.naver.com/sise/sise_market_sum.nhn?sosok=&quot;,i,&quot;&amp;page=&quot;,j) down_table = GET(url) Sys.setlocale(&quot;LC_ALL&quot;, &quot;English&quot;) # 한글 오류 방지를 위해 영어로 로케일 언어 변경 table = read_html(down_table, encoding = &quot;EUC-KR&quot;) %&gt;% html_table(fill = TRUE) table = table[[2]] # 원하는 테이블 추출 Sys.setlocale(&quot;LC_ALL&quot;, &quot;Korean&quot;) # 한글을 읽기위해 로케일 언어 재변경 table[, ncol(table)] = NULL # 토론식 부분 삭제 table = na.omit(table) # 빈 행 삭제 # 6자리 티커만 추출 symbol = read_html(down_table, encoding = &quot;EUC-KR&quot;) %&gt;% html_nodes(., &quot;tbody&quot;) %&gt;% html_nodes(., &quot;td&quot;) %&gt;% html_nodes(., &quot;a&quot;) %&gt;% html_attr(., &quot;href&quot;) symbol = sapply(symbol, function(x) { substr(x, nchar(x) - 5, nchar(x)) }) %&gt;% unique() # 테이블에 티커 넣어준 후, 테이블 정리 table$N = symbol colnames(table)[1] = &quot;종목코드&quot; rownames(table) = NULL ticker[[j]] = table Sys.sleep(0.5) # 페이지 당 0.5초의 슬립 적용 } # do.call을 통해 리스트를 데이터 프레임으로 묶기 ticker = do.call(rbind, ticker) data[[i + 1]] = ticker } # 코스피와 코스닥 테이블 묶기 data = do.call(rbind, data) http://hkconsensus.hankyung.com/↩ http://kind.krx.co.kr/↩ https://finance.naver.com/news/news_list.nhn?mode=LSS2D&amp;section_id=101&amp;section_id2=258↩ "],
["section-17.html", "Chapter 5 금융 데이터 수집하기 (기본) 5.1 한국거래소의 산업별 현황 및 개별지표 크롤링 5.2 WICS 기준 섹터정보 크롤링", " Chapter 5 금융 데이터 수집하기 (기본) API와 크롤링을 이용한다면 비용을 지불하지 않고 얼마든지 금융 데이터를 수집할 수 있습니다. 본 장에서는 금융 데이터를 받기 위해 필요한 주식티커를 구하는 법, 그리고 섹터별 구성종목을 크롤링하는 법에 대해 알아보도록 하겠습니다. 5.1 한국거래소의 산업별 현황 및 개별지표 크롤링 앞 장의 예제를 통해 네이버 금융에서 주식티커 크롤링을 하는 방법에 대해 살펴보았습니다. 그러나 해당 방법은 지나치게 복잡하기도 하며 시간이 오래 걸리기도 합니다. 반면 한국거래소에서 제공하는 산업별 현황과 개별종목 지표 데이터를 이용할 경우 훨씬 간단하게 해당 데이터를 수집할 수 있습니다. 산업별 현황: http://marketdata.krx.co.kr/mdi#document=03030103 개별지표: http://marketdata.krx.co.kr/mdi#document=13020401 물론 해당 데이터들을 크롤링이 아닌 Excel 버튼을 눌러 엑셀로 받을수도 있습니다. 그러나 매번 엑셀을 다운받고 이를 R로 불러오는 작업은 상당히 비효율적이며, 크롤링을 이용한다면 해당 데이터를 효율적으로 불러올 수 있습니다. 5.1.1 산업별 현황 크롤링 먼저 산업별 현황에 해당하는 페이지에 접속한 후, 개발자도구 화면을 연 상태에서 Excel 버튼을 눌러줍니다. Network 탭에는 GenerateOTP.jspx와 download.jspx 총 두가지 항목이 존재합니다. 거래소에서 엑셀 데이터를 받는 과정은 다음과 같습니다. GenerateOTP.jspx: http://marketdata.krx.co.kr/contents/COM/GenerateOTP.jspx에 원하는 항목을 쿼리로 발송하면 해당 쿼리에 해당하는 OTP를 받게 됩니다. download.jspx: 부여받은 OTP를 http://file.krx.co.kr/download.jspx에 제출하면 이에 해당하는 데이터를 다운로드 받게 됩니다. 먼저 1번 단계를 살펴보도록 하겠습니다. Figure 1.1: OTP 생성 부분 General 항목의 Request URL의 앞부분이 원하는 항목을 제출할 주소이며, Query String Parameters에는 우리가 원하는 항목들이 적혀있습니다. 이를 통해 POST 방식으로 데이터를 요청함을 알 수 있습니다. 다음으로 2번 단계를 살펴보도록 하겠습니다. Figure 1.2: OTP 제출 부분 General 항목의 Request URL은 OTP를 제출할 주소이며, Form Data의 OTP는 1번 단계에서 부여받은 OTP에 해당합니다. 이 역시 POST 방식으로 데이터를 요청합니다. 이러한 과정을 코드로 나타내면 다음과 같습니다. library(httr) library(rvest) library(readr) gen_otp_url = &#39;http://marketdata.krx.co.kr/contents/COM/GenerateOTP.jspx&#39; gen_otp_data = list(name = &#39;fileDown&#39;, filetype = &#39;csv&#39;, url = &#39;MKD/03/0303/03030103/mkd03030103&#39;, tp_cd = &#39;ALL&#39;, date = &#39;20190607&#39;, lang = &#39;ko&#39;, pagePath = &#39;/contents/MKD/03/0303/03030103/MKD03030103.jsp&#39;) otp = POST(gen_otp_url, query = gen_otp_data) %&gt;% read_html() %&gt;% html_text() gen_otp_url에 원하는 항목을 제출할 url을 입력합니다. 개발자도구 화면에 나타는 쿼리 내용들을 리스트 형태로 입력합니다. 단, filetype은 기존 xls이 아닌 csv로 변경하여 주며, 이는 csv 형태로 다운로드 받을 경우 데이터를 처리하기 훨씬 쉽기 때문입니다. POST() 함수를 통해 해당 url에 쿼리를 전송하면 이에 해당하는 데이터를 받게 됩니다. read_html()함수를 통해 html 내용을 읽어옵니다. html_text() 함수는 html 내에서 텍스트에 해당하는 부분만을 추출하며, 이를 통해 otp 값만을 추출하게 됩니다. 위의 과정을 거쳐 생성된 OTP는 다음과 같습니다. print(otp) ## [1] &quot;vDfqRc92OziMrxW4rOHJF/ESYC/TDZoj6d0Vg+c5ilkh1GMQN1k9D25XvYBQSwhIOsuiE0ew8pNhWnmku8DpOtiNmXIpso75g4vDUs372LCxSMG6cNImpk8TSE94DPgSk0y0oGVeNVTYiElqlCo8kj6hX+paILvW8fO6BRB+TonPXrmRfymE9VXKhSw4CDjHAznyQYaln6r2ySoQ/BbV3TVUqhmp2JI1uAF08/OmmGPibtwgIMlgnqg+R7erIyhJ78zBrLetbXXfoU6STkYVc9qyLzVHUVqH2/c5IwVxF04=&quot; 이제 생성된 OTP를 제출하면, 우리가 원하는 데이터를 다운로드 받을 수 있습니다. down_url = &#39;http://file.krx.co.kr/download.jspx&#39; down_sector = POST(down_url, query = list(code = otp), add_headers(referer = gen_otp_url)) %&gt;% read_html() %&gt;% html_text() %&gt;% read_csv() OTP를 제출할 url을 down_url에 입력합니다. POST() 함수를 통해 해당 url에 위에서 부여받은 OTP 코드를 제출합니다. add_headers() 구문을 통해 referer를 추가해 주어야 합니다. 리퍼러란 링크를 통해서 각각의 사이트로 방문시 남는 흔적입니다. 데이터를 다운로드 받는 과정을 살펴보면 첫번째 url에서 OTP를 부여 받고, 이를 다시 두번째 url에 제출하였습니다. 그런데 이러한 과정의 흔적이 없이 OTP를 바로 두번째 url에 제출하면 서버는 이를 로봇으로 인식하여 데이터를 반환하지 않습니다. 따라서 add_headers()를 통해 우리가 거쳐온 과정을 흔적으로 남겨야 사람이 데이터를 다운로드 받는 과정과 동일하게 인식하여 데이터를 반환하게 되며, 첫번째 url을 리퍼러로 지정해 줍니다. read_html()과 html_text() 함수를 통해 텍스트 데이터만 추출합니다. read_csv 함수는 csv 형태의 데이터를 불러옵니다. 위의 요청 쿼리에서 filetype을 csv로 지정했기에, 손쉽게 데이터를 읽어올 수 있습니다. print(down_sector) ## # A tibble: 2,243 x 7 ## 시장구분 종목코드 종목명 산업분류 `현재가(종가)` 전일대비 `시가총액(원)` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 코스피 030720 동원수산~ 어업 8940 -20 41605016700 ## 2 코스피 007160 사조산업~ 어업 54400 800 272000000000 ## 3 코스피 006040 동원산업~ 어업 246500 500 829028800000 ## 4 코스피 004970 신라교역~ 어업 14350 350 229600000000 ## 5 코스피 012320 경동인베스~ 광업 40300 1350 95310426900 ## 6 코스피 003580 넥스트사이~ 광업 5200 260 122099322800 ## 7 코스피 017810 풀무원 음식료품 11300 200 430427735000 ## 8 코스피 280360 롯데제과~ 음식료품 159500 -500 1023466361500 ## 9 코스피 271560 오리온 음식료품 83300 1400 3293359795600 ## 10 코스피 006090 사조오양~ 음식료품 8220 30 77454914580 ## # ... with 2,233 more rows 위 과정을 통해 down_sector 변수에는 산업별 현황 데이터가 저장되었습니다. 이를 csv 파일로 다운로드 받도록 하겠습니다. ifelse(dir.exists(&#39;data&#39;), FALSE, dir.create(&#39;data&#39;)) ## [1] FALSE write.csv(down_sector, &#39;data/KOR_sector.csv&#39;) 먼저 ifelse() 함수를 통해 data라는 이름의 폴더가 존재할 시에는 FALSE를 반환, 존재하지 않을 시 해당 이름으로 폴더를 생성하여 줍니다. 그 후, 위에서 다운로드 받은 데이터를 ‘KOR_sector.csv’ 이름으로 저장하여 줍니다. 해당 폴더를 확인해보면, 데이터가 csv 형태로 저장되어 있음이 확인할 수 있습니다. 5.1.2 개별종목 지표 크롤링 개별종목 데이터를 크롤링 하는 방법은 위와 거의 동일하며, 요청하는 쿼리 값에만 차이가 있습니다. 위와 동일하게 개발자도구 화면을 연 상태에서 csv 버튼을 눌러주어 어떠한 쿼리를 요청하는지 확인하도록 합니다. Figure 5.1: 개별지표 OTP 생성 부분 이 중 isu_cdnm, isu_cd, isu_nm, isu_srt_cd, fromdate 항목은 종목구분의 개별탭에 해당하는 부분이므로 우리가 원하는 전체 데이터를 받을때에는 필요하지 않은 요청값입니다. 이를 제외한 요청값을 산업별 현황 예제에 적용하면 해당 데이터 역시 손쉽게 다운로드 받을 수 있습니다. library(httr) library(rvest) library(readr) gen_otp_url = &#39;http://marketdata.krx.co.kr/contents/COM/GenerateOTP.jspx&#39; gen_otp_data = list(name = &#39;fileDown&#39;, filetype = &#39;csv&#39;, url = &quot;MKD/13/1302/13020401/mkd13020401&quot;, market_gubun = &#39;ALL&#39;, gubun = &#39;1&#39;, schdate = &#39;20190607&#39;, pagePath = &quot;/contents/MKD/13/1302/13020401/MKD13020401.jsp&quot;) otp = POST(gen_otp_url, query = gen_otp_data) %&gt;% read_html() %&gt;% html_text() down_url = &#39;http://file.krx.co.kr/download.jspx&#39; down_ind = POST(down_url, query = list(code = otp), add_headers(referer = gen_otp_url)) %&gt;% read_html() %&gt;% html_text() %&gt;% read_csv() print(down_ind) ## # A tibble: 2,204 x 13 ## 일자 종목코드 종목명 관리여부 종가 EPS PER BPS PBR ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2019-06-07 060300 레드로버~ - 1190 - - 2,128 0.56 ## 2 2019-06-07 290650 엘앤씨바이~ - 22750 830 27.41 7,252 3.14 ## 3 2019-06-07 239340 미래에셋제~ - 5200 - - - - ## 4 2019-06-07 086060 진바이오텍~ - 6940 312 22.24 5,570 1.25 ## 5 2019-06-07 033430 디에스티~ - 1310 - - 338 3.88 ## 6 2019-06-07 038680 에스넷 - 9600 300 32 4,302 2.23 ## 7 2019-06-07 214680 디알텍 - 2005 29 69.14 709 2.83 ## 8 2019-06-07 242040 나무기술~ - 3345 - - 462 7.24 ## 9 2019-06-07 121890 에스디시스~ - 4980 - - 1,912 2.6 ## 10 2019-06-07 088800 에이스테크~ - 11500 97 118.~ 2,291 5.02 ## # ... with 2,194 more rows, and 4 more variables: 주당배당금 &lt;dbl&gt;, ## # 배당수익률 &lt;dbl&gt;, `게시물 일련번호` &lt;dbl&gt;, 총카운트 &lt;dbl&gt; 위 과정을 통해 down_ind 변수에는 개별종목 지표 데이터가 저장되었습니다. 해당 데이터 역시 csv 파일로 다운로드 받도록 하겠습니다. write.csv(down_ind, &#39;data/KOR_ind.csv&#39;) 위 예제의 쿼리 항목 중 date와 schdate 부분만 원하는 일자로 입력할 경우(예: 20190104), 해당일의 데이터를 다운로드 받을 수 있습니다. 단, 휴장일을 입력할 경우 데이터를 받을 수 없습니다. 5.1.3 데이터 정리하기 위에서 다운받은 데이터는 중복된 열이 있으며, 불필요한 데이터 역시 존재합니다. 따라서 하나의 테이블로 합쳐준 후 정리를 할 필요가 있습니다. 먼저 다운로드 받은 csv 파일을 읽어오도록 합니다. down_sector = read.csv(&#39;data/KOR_sector.csv&#39;, row.names = 1, stringsAsFactors = FALSE) down_ind = read.csv(&#39;data/KOR_ind.csv&#39;, row.names = 1, stringsAsFactors = FALSE) read.csv() 함수를 이용하여 데이터를 읽어오며, row.names = 1를 통해 첫번째 열을 행이름으로, stringsAsFactors = FALSE를 통해 캐릭터 데이터가 팩터 형태로 변형되지 않게 합니다. intersect(names(down_sector), names(down_ind)) ## [1] &quot;종목코드&quot; &quot;종목명&quot; 먼저 intersect() 함수를 통해 두 데이터간 중복되는 열이름을 상펴보면, 종목코드와 종목명이 동일하게 위치합니다. setdiff(down_sector[, &#39;종목명&#39;], down_ind[ ,&#39;종목명&#39;]) ## [1] &quot;엘브이엠씨홀딩스&quot; &quot;한국패러랠&quot; &quot;한국ANKOR유전&quot; ## [4] &quot;맵스리얼티1&quot; &quot;맥쿼리인프라&quot; &quot;하나니켈2호&quot; ## [7] &quot;하나니켈1호&quot; &quot;베트남개발1&quot; &quot;신한알파리츠&quot; ## [10] &quot;이리츠코크렙&quot; &quot;모두투어리츠&quot; &quot;하이골드12호&quot; ## [13] &quot;하이골드8호&quot; &quot;바다로19호&quot; &quot;하이골드3호&quot; ## [16] &quot;케이탑리츠&quot; &quot;에이리츠&quot; &quot;동북아13호&quot; ## [19] &quot;동북아12호&quot; &quot;컬러레이&quot; &quot;JTC&quot; ## [22] &quot;뉴프라이드&quot; &quot;윙입푸드&quot; &quot;글로벌에스엠&quot; ## [25] &quot;크리스탈신소재&quot; &quot;씨케이에이치&quot; &quot;차이나그레이트&quot; ## [28] &quot;골든센츄리&quot; &quot;오가닉티코스메틱&quot; &quot;GRT&quot; ## [31] &quot;로스웰&quot; &quot;헝셩그룹&quot; &quot;이스트아시아홀딩스&quot; ## [34] &quot;에스앤씨엔진그룹&quot; &quot;SNK&quot; &quot;SBI핀테크솔루션즈&quot; ## [37] &quot;잉글우드랩&quot; &quot;코오롱티슈진&quot; &quot;엑세스바이오&quot; setdiff() 함수를 통해 두 데이터에 공통적으로 존재하지 않는 종목명, 즉 하나의 데이터에만 존재하는 종목을 살펴보면 위와 같습니다. 해당 종목들은 선박펀드, 광물펀드, 해외종목 등 일반적이지 않은 종목들이므로, 제외해주는 것이 좋습니다. 따라서 둘간에 공통적으로 존재하는 종목을 기준으로 데이터를 합쳐주도록 하겠습니다. KOR_ticker = merge(down_sector, down_ind, by = intersect(names(down_sector), names(down_ind)), all = FALSE ) merge() 함수는 by를 기준으로 하여 두 데이터를 하나로 합치게 되며, 그 기준은 공통으로 존재하는 열이름으로 합니다. 또한 all 값을 TRUE로 설정할 경우는 합집합을, FALSE로 설정할 경우 교집합을 반환하며, 공통적으로 존재하는 항목을 원하므로 FALSE를 선택해 주도록 합니다. KOR_ticker = KOR_ticker[order(-KOR_ticker[&#39;시가총액.원.&#39;]), ] print(head(KOR_ticker)) ## 종목코드 종목명 시장구분 산업분류 현재가.종가. 전일대비 ## 330 005930 삼성전자 코스피 전기전자 44200 300 ## 45 000660 SK하이닉스 코스피 전기전자 65400 300 ## 301 005380 현대차 코스피 운수장비 140000 -1000 ## 331 005935 삼성전자우 코스피 전기전자 36250 200 ## 1278 068270 셀트리온 코스피 의약품 196500 500 ## 1082 051910 LG화학 코스피 화학 330500 -1000 ## 시가총액.원. 일자 관리여부 종가 EPS PER BPS PBR ## 330 2.638644e+14 2019-06-07 - 44200 6,461 6.84 35,342 1.25 ## 45 4.761135e+13 2019-06-07 - 65400 22,255 2.94 64,348 1.02 ## 301 2.991355e+13 2019-06-07 - 140000 5,632 24.86 245,447 0.57 ## 331 2.982964e+13 2019-06-07 - 36250 - - - - ## 1278 2.521666e+13 2019-06-07 - 196500 2,063 95.25 19,766 9.94 ## 1082 2.333077e+13 2019-06-07 - 330500 19,217 17.2 218,227 1.51 ## 주당배당금 배당수익률 게시물..일련번호 총카운트 ## 330 1416 3.20 1456 NA ## 45 1500 2.29 1408 NA ## 301 4000 2.86 1450 NA ## 331 1417 3.91 2085 NA ## 1278 0 0.00 1595 NA ## 1082 6000 1.82 1536 NA 데이터를 시가총액 순으로 내림차순 해줄 필요도 있습니다. order() 함수를 통해 상대적인 순서를 구할 수 있으며, R은 기본적으로 오름차순으로 순서를 구하므로 앞에 마이너스(-)를 붙여 내림차순 형태로 바꾸어 주도록 합니다. 결과적으로 시가총액 기준 내림차순으로 해당 데이터가 정렬됩니다. 마지막으로 스팩, 우선주 종목 역시 제외해 주어야 합니다. KOR_ticker[grepl(&#39;스팩&#39;, KOR_ticker[, &#39;종목명&#39;]), &#39;종목명&#39;] # 스팩 ## [1] &quot;미래에셋제5호스팩&quot; &quot;한화에스비아이스팩&quot; &quot;엔에이치스팩14호&quot; ## [4] &quot;엔에이치스팩10호&quot; &quot;엔에이치스팩12호&quot; &quot;삼성스팩2호&quot; ## [7] &quot;대신밸런스제5호스팩&quot; &quot;케이비제10호스팩&quot; &quot;엔에이치스팩11호&quot; ## [10] &quot;유진스팩4호&quot; &quot;신한제4호스팩&quot; &quot;하나금융11호스팩&quot; ## [13] &quot;케이비17호스팩&quot; &quot;DB금융스팩7호&quot; &quot;SK4호스팩&quot; ## [16] &quot;한국제7호스팩&quot; &quot;대신밸런스제6호스팩&quot; &quot;미래에셋대우스팩1호&quot; ## [19] &quot;대신밸런스제4호스팩&quot; &quot;IBKS제5호스팩&quot; &quot;동부스팩5호&quot; ## [22] &quot;DB금융스팩6호&quot; &quot;하나머스트제6호스팩&quot; &quot;하나금융10호스팩&quot; ## [25] &quot;삼성머스트스팩3호&quot; &quot;유안타제4호스팩&quot; &quot;한국제6호스팩&quot; ## [28] &quot;IBKS제6호스팩&quot; &quot;신영스팩4호&quot; &quot;한화수성스팩&quot; ## [31] &quot;IBKS제10호스팩&quot; &quot;하이제4호스팩&quot; &quot;하나금융9호스팩&quot; ## [34] &quot;유안타제3호스팩&quot; &quot;교보7호스팩&quot; &quot;한국제5호스팩&quot; ## [37] &quot;IBKS제9호스팩&quot; &quot;교보8호스팩&quot; &quot;IBKS제7호스팩&quot; ## [40] &quot;신한제3호스팩&quot; &quot;키움제5호스팩&quot; &quot;SK3호스팩&quot; ## [43] &quot;한국제8호스팩&quot; &quot;한화에이스스팩4호&quot; &quot;엔에이치스팩13호&quot; ## [46] &quot;미래에셋대우스팩2호&quot; &quot;한화에이스스팩3호&quot; &quot;케이비제11호스팩&quot; KOR_ticker[KOR_ticker[, &#39;종목명&#39;] == &#39;골든브릿지이안5호&#39;, &#39;종목명&#39;] # 골든브릿지이안5호 ## [1] &quot;골든브릿지이안5호&quot; KOR_ticker[substr(KOR_ticker[, &#39;종목명&#39;], nchar(KOR_ticker[,&#39;종목명&#39;]), nchar(KOR_ticker[,&#39;종목명&#39;])) == &#39;우&#39;, &#39;종목명&#39;] ## [1] &quot;삼성전자우&quot; &quot;미래에셋대우&quot; &quot;현대차우&quot; ## [4] &quot;LG생활건강우&quot; &quot;LG화학우&quot; &quot;아모레퍼시픽우&quot; ## [7] &quot;삼성화재우&quot; &quot;LG전자우&quot; &quot;신영증권우&quot; ## [10] &quot;연우&quot; &quot;두산우&quot; &quot;한국금융지주우&quot; ## [13] &quot;대신증권우&quot; &quot;S-Oil우&quot; &quot;대림산업우&quot; ## [16] &quot;NH투자증권우&quot; &quot;아모레G우&quot; &quot;CJ제일제당 우&quot; ## [19] &quot;LG우&quot; &quot;SK이노베이션우&quot; &quot;삼성SDI우&quot; ## [22] &quot;삼성전기우&quot; &quot;CJ우&quot; &quot;금호석유우&quot; ## [25] &quot;SK우&quot; &quot;GS우&quot; &quot;미래에셋대우우&quot; ## [28] &quot;롯데칠성우&quot; &quot;부국증권우&quot; &quot;코오롱인더우&quot; ## [31] &quot;롯데지주우&quot; &quot;유한양행우&quot; &quot;유화증권우&quot; ## [34] &quot;호텔신라우&quot; &quot;SK케미칼우&quot; &quot;남양유업우&quot; ## [37] &quot;한진칼우&quot; &quot;LG하우시스우&quot; &quot;BYC우&quot; ## [40] &quot;유안타증권우&quot; &quot;대한항공우&quot; &quot;세방우&quot; ## [43] &quot;SK디스커버리우&quot; &quot;대덕전자1우&quot; &quot;태영건설우&quot; ## [46] &quot;대상우&quot; &quot;금호산업우&quot; &quot;한화우&quot; ## [49] &quot;현대건설우&quot; &quot;한화케미칼우&quot; &quot;삼양홀딩스우&quot; ## [52] &quot;녹십자홀딩스2우&quot; &quot;신풍제약우&quot; &quot;삼양사우&quot; ## [55] &quot;넥센우&quot; &quot;SK증권우&quot; &quot;남선알미우&quot; ## [58] &quot;코오롱우&quot; &quot;계양전기우&quot; &quot;NPC우&quot; ## [61] &quot;한화투자증권우&quot; &quot;SK네트웍스우&quot; &quot;태양금속우&quot; ## [64] &quot;쌍용양회우&quot; &quot;서울식품우&quot; &quot;성문전자우&quot; ## [67] &quot;대원전선우&quot; &quot;일양약품우&quot; &quot;성신양회우&quot; ## [70] &quot;대한제당우&quot; &quot;유유제약1우&quot; &quot;코리아써우&quot; ## [73] &quot;크라운해태홀딩스우&quot; &quot;동원시스템즈우&quot; &quot;CJ씨푸드1우&quot; ## [76] &quot;현대비앤지스틸우&quot; &quot;삼성중공우&quot; &quot;금강공업우&quot; ## [79] &quot;대호피앤씨우&quot; &quot;크라운제과우&quot; &quot;동부제철우&quot; ## [82] &quot;깨끗한나라우&quot; &quot;노루페인트우&quot; &quot;대상홀딩스우&quot; ## [85] &quot;덕성우&quot; &quot;코오롱글로벌우&quot; &quot;동양우&quot; ## [88] &quot;신원우&quot; &quot;DB하이텍1우&quot; &quot;하이트진로홀딩스우&quot; ## [91] &quot;흥국화재우&quot; &quot;JW중외제약우&quot; &quot;한양증권우&quot; ## [94] &quot;동부건설우&quot; &quot;노루홀딩스우&quot; &quot;소프트센우&quot; KOR_ticker[substr(KOR_ticker[, &#39;종목명&#39;], nchar(KOR_ticker[,&#39;종목명&#39;]) -1, nchar(KOR_ticker[,&#39;종목명&#39;])) == &#39;우B&#39;, &#39;종목명&#39;] ## [1] &quot;현대차2우B&quot; &quot;미래에셋대우2우B&quot; &quot;한화3우B&quot; ## [4] &quot;현대차3우B&quot; &quot;삼성물산우B&quot; &quot;대교우B&quot; ## [7] &quot;대신증권2우B&quot; &quot;두산2우B&quot; &quot;넥센타이어1우B&quot; ## [10] &quot;하이트진로2우B&quot; &quot;진흥기업2우B&quot; &quot;진흥기업우B&quot; ## [13] &quot;JW중외제약2우B&quot; &quot;흥국화재2우B&quot; &quot;코리아써키트2우B&quot; ## [16] &quot;유유제약2우B&quot; &quot;동양2우B&quot; &quot;대한제당3우B&quot; ## [19] &quot;동양3우B&quot; KOR_ticker[substr(KOR_ticker[, &#39;종목명&#39;], nchar(KOR_ticker[,&#39;종목명&#39;]) -1, nchar(KOR_ticker[,&#39;종목명&#39;])) == &#39;우C&#39;, &#39;종목명&#39;] ## [1] &quot;루트로닉3우C&quot; grepl() 함수를 통해 종목명에 ‘스팩’이 들어가는 종목, 스팩 종목인 ’골든브릿지이안5호’, substr() 함수를 통해 종목명 끝이 ‘우’, ‘우B’, ’우C’인 우선주 종목을 찾을 수 있습니다. 데이터 내에서 해당 데이터들을 제거13해 주도록 하겠습니다. KOR_ticker = KOR_ticker[!grepl(&#39;스팩&#39;, KOR_ticker[, &#39;종목명&#39;]), ] # 스팩 KOR_ticker = KOR_ticker[KOR_ticker[, &#39;종목명&#39;] != &#39;골든브릿지이안5호&#39;, ] # 골든브릿지이안5호 KOR_ticker = KOR_ticker[substr(KOR_ticker[, &#39;종목명&#39;], nchar(KOR_ticker[,&#39;종목명&#39;]), nchar(KOR_ticker[,&#39;종목명&#39;])) != &#39;우&#39;, ] KOR_ticker = KOR_ticker[substr(KOR_ticker[, &#39;종목명&#39;], nchar(KOR_ticker[,&#39;종목명&#39;]) -1, nchar(KOR_ticker[,&#39;종목명&#39;])) != &#39;우B&#39;, ] KOR_ticker = KOR_ticker[substr(KOR_ticker[, &#39;종목명&#39;], nchar(KOR_ticker[,&#39;종목명&#39;]) -1, nchar(KOR_ticker[,&#39;종목명&#39;])) != &#39;우C&#39;, ] 마지막으로 행이름을 초기화 한 후, 정리된 데이터를 csv 파일로 저장해주도록 합니다. rownames(KOR_ticker) = NULL write.csv(KOR_ticker, &#39;data/KOR_ticker.csv&#39;) 5.2 WICS 기준 섹터정보 크롤링 일반적으로 주식의 섹터를 나누는 기준은 MSCI와 S&amp;P가 개발한 GICS14를 가장 많이 사용합니다. 국내 종목의 GICS 기준 정보 역시 한국거래소에서 제공하고 있으나, 이는 독점적 지적재산으로 명시했기에 사용하는데 무리가 있습니다. 그러나 지수제공업체인 와이즈인덱스15에서는 GICS와 비슷한 WICS 산업분류를 발표하고 있으므로, 이를 크롤링하여 필요한 정보를 수집해보도록 하겠습니다. 먼저, 웹페이지에 접속하여 Index → WISE SECTOR INDEX → WICS → 에너지를 클릭합니다. 그 후 Components 탭을 클릭하면, 해당 섹터의 구성종목을 확인할 수 있습니다. Figure 2.8: WICS 기준 구성종목 개발자도구 화면을 통해 해당 페이지의 데이터전송 과정을 살펴보도록 하겠습니다. Figure 5.2: WICS 페이지 개발자도구 화면 일자를 선택하면 Network 탭의 GetIndexComponets 항목을 통해 데이터 전송과정이 나타나며, Request URL의 주소를 살펴보면 다음과 같습니다. http://www.wiseindex.com/Index/GetIndexComponets?ceil_yn=0&amp;dt=20190607&amp;sec_cd=G10 http://www.wiseindex.com/Index/GetIndexComponets: 데이터를 요청하는 url 입니다. ceil_yn = 0: 실링여부를 나타내며, 0일 경우 비실링을 의미합니다. dt=20190607: 조회일자를 나타냅니다. sec_cd=G10: 섹터 코드를 나타냅니다. 이번엔 위 주소의 페이지를 열어보도록 하겠습니다. Figure 2.9: WICS 데이터 페이지 글자들을 살펴보면 페이지에 출력된 내용임을 알 수 있지만, 매우 특이한 형태로 구성되어 있습니다. 이러한 형식이 데이터를 json 형식이라 합니다. 기존에 우리가 살펴보았던 대부분의 웹페이지는 XML 형식으로 표현되었으며, 이는 문법이 복잡하고 엄격한 표현규칙으로 인해 데이터의 용량이 커진다는 단점이 있습니다. 반면 JSON 형식은 문법이 단순하여 데이터의 용량이 작아, 빠른 속도로 데이터를 교환할 수 있습니다. R에서는 jsonlite 패키지의 fromJSON() 함수를 사용하여 매우 손쉽게 해당 형태의 데이터를 크롤링할 수 있습니다. library(jsonlite) url = &#39;http://www.wiseindex.com/Index/GetIndexComponets?ceil_yn=0&amp;dt=20190607&amp;sec_cd=G10&#39; data = fromJSON(url) print(data) ## $info ## $info$TRD_DT ## [1] &quot;/Date(1559833200000)/&quot; ## ## $info$MKT_VAL ## [1] 19850082 ## ## $info$TRD_AMT ## [1] 70030 ## ## $info$CNT ## [1] 23 ## ## ## $list ## IDX_CD IDX_NM_KOR ALL_MKT_VAL CMP_CD CMP_KOR MKT_VAL WGT ## 1 G10 WICS 에너지 19850082 096770 SK이노베이션 9052841 45.61 ## 2 G10 WICS 에너지 19850082 010950 S-Oil 3403265 17.14 ## 3 G10 WICS 에너지 19850082 267250 현대중공업지주 2873204 14.47 ## 4 G10 WICS 에너지 19850082 078930 GS 2491805 12.55 ## 5 G10 WICS 에너지 19850082 067630 에이치엘비생명과학 624986 3.15 ## 6 G10 WICS 에너지 19850082 006120 SK디스커버리 257059 1.30 ## 7 G10 WICS 에너지 19850082 002960 한국쉘석유 198881 1.00 ## 8 G10 WICS 에너지 19850082 091090 세원셀론텍 146439 0.74 ## 9 G10 WICS 에너지 19850082 011930 신성이엔지 118388 0.60 ## 10 G10 WICS 에너지 19850082 128820 대성산업 79162 0.40 ## 11 G10 WICS 에너지 19850082 043200 파루 75903 0.38 ## 12 G10 WICS 에너지 19850082 003650 미창석유 71918 0.36 ## 13 G10 WICS 에너지 19850082 014530 극동유화 62828 0.32 ## 14 G10 WICS 에너지 19850082 099220 SDN 55967 0.28 ## 15 G10 WICS 에너지 19850082 024060 흥구석유 53460 0.27 ## 16 G10 WICS 에너지 19850082 041590 에너전트 50901 0.26 ## 17 G10 WICS 에너지 19850082 095910 에스에너지 46873 0.24 ## 18 G10 WICS 에너지 19850082 060900 퍼시픽바이오 43739 0.22 ## 19 G10 WICS 에너지 19850082 012320 경동인베스트 40030 0.20 ## 20 G10 WICS 에너지 19850082 093230 이아이디 32266 0.16 ## 21 G10 WICS 에너지 19850082 038620 위즈코프 30094 0.15 ## 22 G10 WICS 에너지 19850082 137950 제이씨케미칼 29363 0.15 ## 23 G10 WICS 에너지 19850082 000440 중앙에너비스 10711 0.05 ## S_WGT CAL_WGT SEC_CD SEC_NM_KOR SEQ TOP60 APT_SHR_CNT ## 1 45.61 1 G10 에너지 1 2 56403994 ## 2 62.75 1 G10 에너지 2 2 41655633 ## 3 77.23 1 G10 에너지 3 2 9283372 ## 4 89.78 1 G10 에너지 4 2 49245150 ## 5 92.93 1 G10 에너지 5 2 39307272 ## 6 94.22 1 G10 에너지 6 2 10470820 ## 7 95.22 1 G10 에너지 7 2 611000 ## 8 95.96 1 G10 에너지 8 2 42693554 ## 9 96.56 1 G10 에너지 9 2 100756131 ## 10 96.96 1 G10 에너지 10 2 15832417 ## 11 97.34 1 G10 에너지 11 2 29707590 ## 12 97.70 1 G10 에너지 12 2 922026 ## 13 98.02 1 G10 에너지 13 2 18132098 ## 14 98.30 1 G10 에너지 14 2 38598075 ## 15 98.57 1 G10 에너지 15 2 9000000 ## 16 98.83 1 G10 에너지 16 2 31132353 ## 17 99.06 1 G10 에너지 17 2 10290527 ## 18 99.28 1 G10 에너지 18 2 53017111 ## 19 99.48 1 G10 에너지 19 2 993310 ## 20 99.65 1 G10 에너지 20 2 140285009 ## 21 99.80 1 G10 에너지 21 2 24075079 ## 22 99.95 1 G10 에너지 22 2 8353752 ## 23 100.00 1 G10 에너지 23 2 1556783 ## ## $sector ## SEC_CD SEC_NM_KOR SEC_RATE IDX_RATE ## 1 G25 경기관련소비재 16.05 0 ## 2 G35 건강관리 9.27 0 ## 3 G50 전기통신서비스 2.26 0 ## 4 G40 금융 10.31 0 ## 5 G10 에너지 2.37 100 ## 6 G20 산업재 12.68 0 ## 7 G55 유틸리티 1.70 0 ## 8 G30 필수소비재 3.75 0 ## 9 G15 소재 7.89 0 ## 10 G45 IT 33.73 0 ## ## $size ## SEC_CD SEC_NM_KOR SEC_RATE IDX_RATE ## 1 WMI510 WMI500 대형주 69.40 89.78 ## 2 WMI520 WMI500 중형주 13.56 4.44 ## 3 WMI530 WMI500 소형주 17.04 5.78 $list 항목에는 해당 섹터의 구성종목 정보가 있으며, $sector 항목을 통해 다른 섹터들의 코드도 확인할 수 있습니다. for loop 구문을 이용해 url의 sec_cd=에 해당하는 부분만 변경해주면, 모든 섹터의 구성종목을 매우 쉽게 얻을 수 있습니다. sector_code = c(&#39;G25&#39;, &#39;G35&#39;, &#39;G50&#39;, &#39;G40&#39;, &#39;G10&#39;, &#39;G20&#39;, &#39;G55&#39;, &#39;G30&#39;, &#39;G15&#39;, &#39;G45&#39;) data_sector = list() for (i in sector_code) { url = paste0( &#39;http://www.wiseindex.com/Index/GetIndexComponets?ceil_yn=0&amp;dt=20190607&amp;sec_cd=&#39;,i) data = fromJSON(url) data = data$list data_sector[[i]] = data Sys.sleep(1) } data_sector = do.call(rbind, data_sector) 해당 데이터 역시 csv 파일로 저장해주도록 합니다. write.csv(data_sector, &#39;data/KOR_sector.csv&#39;) 해당 과정에서 미래에셋대우, 연우 등 의도치 않은 종목 역시 제거됩니다. 그러나 이러한 종목수가 그리 많지 않으므로 투자에 있어 중요하지는 않습니다.↩ https://en.wikipedia.org/wiki/Global_Industry_Classification_Standard↩ http://www.wiseindex.com/↩ "],
["section-22.html", "Chapter 6 금융 데이터 수집하기 (심화) 6.1 수정주가 크롤링 6.2 재무제표 및 가치지표 크롤링 6.3 야후 파이낸스 데이터 구하기", " Chapter 6 금융 데이터 수집하기 (심화) 지난 장에서는 주식티커를 구하였습니다. 이를 바탕으로 이번 장에서는 퀀트 투자의 핵심 자료인 수정주가, 재무제표 및 가치지표를 크롤링 하는법에 대해 알아보도록 하겠습니다. 6.1 수정주가 크롤링 주가 데이터는 투자를 함에 있어 반드시 필요한 데이터이며, 인터넷에서 주가를 수집할 수 있는 방법은 매우 많습니다. 먼저, API를 이용한 데이터 수집에서 살펴본 것과 같이, getSymbols() 함수를 이용하여 데이터를 받을 수 있습니다. 그러나 야후 파이낸스에서 제공하는 데이터의 경우 미국 주가는 이상없이 다운로드가 되지만, 국내 중소형주의 경우 주가가 없는 경우가 있습니다. 또한 단순 주가를 구할수 있는 방법은 많지만, 투자에 필요한 수정주가를 구할 수 있는 방법은 찾기 힘듭니다. 다행히 네이버 금융에서 제공하는 정보를 통해 모든 종목의 수정주가를 매우 손쉽게 구할 수 있습니다. 6.1.1 개별 종목 주가 크롤링 먼저 네이버 금융에서 특정종목(예: 삼성전자)의 차트 탭16을 선택합니다. Figure 1.1: 네이버금융 개별종목 차트탭 위와 같이 플래쉬가 차단되어 화면이 나오지 않는 경우, 주소창의 왼쪽 상단에 위치한 자물쇠 버튼을 클릭한 다음, Flash를 허용으로 바꾼 후 새로고침을 누르면 차트가 나오게 됩니다. 해당 차트는 주가 데이터를 받아 그림을 그려주는 형태입니다. 따라서 해당 데이터가 어디에서 오는지 알기 위해 개발자도구 화면을 이용하도록 합니다. Figure 1.2: 네이버금융 차트의 통신기록 화면을 연 상태에서 일봉 탭을 선택하면 sise.nhn, schedule.nhn, notice.nhn 총 3가지 항목이 생성됩니다. 이 중 sise.nhn 항목의 Request URL이 주가 데이터를 요청하는 주소입니다. 해당 url에 접속해 보도록 하겠습니다. Figure 1.3: 주가 데이터 페이지 각 날짜별로 시가, 고가, 저가, 종가, 거래량이 있으며, 주가의 경우 모두 수정주가 기준입니다. 또한 해당 데이터가 item 태그 내 data 속성에 위치하고 있습니다. 위 주소에서 symbol= 뒤에 위치하는 6자리 티커만 변경하면 해당 종목의 주가 데이터가 위치한 페이지로 이동할 수 있으며, 우리가 원하는 모든 종목들의 주가 데이터를 크롤링 할 수 있습니다. 이러한 티커의 경우 우리는 이미 거래소에서 받은 데이터를 통해 가지고 있습니다. library(stringr) KOR_ticker = read.csv(&#39;data/KOR_ticker.csv&#39;, row.names = 1) print(KOR_ticker$&#39;종목코드&#39;[1]) ## [1] 5930 KOR_ticker$&#39;종목코드&#39; = str_pad(KOR_ticker$&#39;종목코드&#39;, 6, side = c(&#39;left&#39;), pad = &#39;0&#39;) 먼저 저장해두었던 csv 파일을 불러오도록 합니다. 종목코드를 살펴보면 005930 이어야 할 삼성전자의 티커가 5930으로 입력되어 있습니다. 이는 파일을 불러오는 과정에서 0으로 시작하는 숫자들이 지워져버렸기 때문입니다. 이를 6자리로 다시 맞춰주기 위해, stringr 패키지의 str_pad() 함수를 사용해 줍니다. 6자리가 되지 않는 문자는, 왼쪽에 0을 추가하여 강제로 6자리로 만들어 주도록 합니다. 다음은 첫번째 종목인 삼성전자의 주가를 크롤링한 후 가공하는 방법입니다. library(xts) ifelse(dir.exists(&#39;data/KOR_price&#39;), FALSE, dir.create(&#39;data/KOR_price&#39;)) ## [1] FALSE i = 1 name = KOR_ticker$&#39;종목코드&#39;[i] price = xts(NA, order.by = Sys.Date()) print(price) ## [,1] ## 2019-06-13 NA 먼저 data 폴더 내에 KOR_price 폴더를 생성해줍니다. i = 1 을 입력해 줍니다. 향후 for loop 구문을 통해 i 값만 변경하면 모든 종목의 주가를 다운로드 받을 수 있습니다. name에 해당 티커를 입력해줍니다. xts() 함수를 이용해 빈 시계열 데이터를 생성해주며, 인덱스는 Sys.Date()를 통해 현재 날짜를 입력합니다. library(httr) library(rvest) url = paste0(&#39;https://fchart.stock.naver.com/sise.nhn?symbol=&#39;, name,&#39;&amp;timeframe=day&amp;count=500&amp;requestType=0&#39;) data = GET(url) data_html = read_html(data, encoding = &#39;EUC-KR&#39;) %&gt;% html_nodes(&#39;item&#39;) %&gt;% html_attr(&#39;data&#39;) print(head(data_html)) ## [1] &quot;20170526|45600|46460|45540|46080|272273&quot; ## [2] &quot;20170529|46220|46400|45380|45620|174791&quot; ## [3] &quot;20170530|45520|45660|44480|44640|248672&quot; ## [4] &quot;20170531|44580|45020|44400|44700|373382&quot; ## [5] &quot;20170601|44860|44900|44400|44680|195070&quot; ## [6] &quot;20170602|45060|45960|45000|45960|249775&quot; paste0() 함수를 이용해 원하는 종목의 url을 생성해 줍니다. url중 티커에 해당하는 6자리 부분만 위에서 입력한 name으로 설정해주면 됩니다. GET() 함수를 통해 페이지의 데이터를 불러옵니다. read_html() 함수를 통해 html 정보를 읽어옵니다. html_nodes()와 html_attr() 함수를 통해 item 태그 및 data 속성의 데이터를 추출합니다. 결과적으로 날짜 및 주가, 거래량 데이터만 추출되어 집니다. 해당 데이터는 ’|’으로 구분되어 있으며, 이를 테이블 형태로 바꿀 필요가 있습니다. library(readr) price = read_delim(data_html, delim = &#39;|&#39;) print(head(price)) ## # A tibble: 6 x 6 ## `20170526` `45600` `46460` `45540` `46080` `272273` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 20170529 46220 46400 45380 45620 174791 ## 2 20170530 45520 45660 44480 44640 248672 ## 3 20170531 44580 45020 44400 44700 373382 ## 4 20170601 44860 44900 44400 44680 195070 ## 5 20170602 45060 45960 45000 45960 249775 ## 6 20170605 46040 46360 45720 45940 151988 readr 패키지의 read_delim() 함수를 쓸 경우 구분자로 이루어진 데이터를 테이블로 쉽게 변경할 수 있습니다. 데이터를 확인해보면 테이블 형태로 변경되었으며 각 열은 날짜, 시가, 고가, 저가, 종가, 거래량을 의미합니다. 이 중 우리가 필요한 날짜와 종가를 선택한 후, 데이터 클랜징을 해주도록 합니다. library(lubridate) price = price[c(1, 5)] price = data.frame(price) colnames(price) = c(&#39;Date&#39;, &#39;Price&#39;) price[, 1] = ymd(price[, 1]) rownames(price) = price[, 1] price[, 1] = NULL print(tail(price)) ## Price ## 2019-06-05 43900 ## 2019-06-07 44200 ## 2019-06-10 44800 ## 2019-06-11 44850 ## 2019-06-12 44600 ## 2019-06-13 43750 날짜에 해당하는 첫번째 열과, 종가에 해당하는 다섯번째 열만을 선택해 저장해 줍니다. 티블 형태의 데이터를 데이터프레임 형태로 변경해 줍니다. 열이름을 Date와 Price로 변경해 줍니다. lubridate 패키지의 ymd() 함수를 이용하여 yyyymmdd 형태를 yyyy-mm-dd로 변경해주며, 데이터 형태 또한 Date 타입으로 변경됩니다. 행이름을 첫번째 열인 날짜로 변경해준 후, 해당 열은 NULL을 통해 삭제해 줍니다. 데이터를 확인해보면 우리에게 필요한 형태로 정리가 되었습니다. 마지막으로 해당 데이터를 csv 파일로 저장해주도록 합니다. write.csv(price, paste0(&#39;data/KOR_price/&#39;, name, &#39;_price.csv&#39;)) data 폴더의 KOR_price 폴더 내에 티커_price.csv 이름으로 저장해주도록 합니다. 6.1.2 전 종목 주가 크롤링 위의 코드에서 for loop 구문을 이용하여 i 값만 변경해주면 모든 종목의 주가를 다운로드 받을 수 있습니다. 전 종목 주가를 다운로드 받는 전체 코드는 다음과 같습니다. library(httr) library(rvest) library(stringr) library(xts) library(lubridate) KOR_ticker = read.csv(&#39;data/KOR_ticker.csv&#39;, row.names = 1) print(KOR_ticker$&#39;종목코드&#39;[1]) KOR_ticker$&#39;종목코드&#39; = str_pad(KOR_ticker$&#39;종목코드&#39;, 6, side = c(&#39;left&#39;), pad = &#39;0&#39;) ifelse(dir.exists(&#39;data/KOR_price&#39;), FALSE, dir.create(&#39;data/KOR_price&#39;)) for(i in 1 : nrow(KOR_ticker) ) { price = xts(NA, order.by = Sys.Date()) # 빈 시계열 데이터 생성 name = KOR_ticker$&#39;종목코드&#39;[i] # 티커 부분 선택 # 오류 발생 시 이를 무시하고 다음 루프로 진행 tryCatch({ # url 생성 url = paste0(&#39;https://fchart.stock.naver.com/sise.nhn?symbol=&#39; ,name,&#39;&amp;timeframe=day&amp;count=500&amp;requestType=0&#39;) # 이 후 과정은 위와 동일함 # 데이터 다운로드 data = GET(url) data_html = read_html(data, encoding = &#39;EUC-KR&#39;) %&gt;% html_nodes(&quot;item&quot;) %&gt;% html_attr(&quot;data&quot;) # 데이터 나누기 price = read_delim(data_html, delim = &#39;|&#39;) # 필요한 열만 선택 후 클렌징 price = price[c(1, 5)] price = data.frame(price) colnames(price) = c(&#39;Date&#39;, &#39;Price&#39;) price[, 1] = ymd(price[, 1]) rownames(price) = price[, 1] price[, 1] = NULL }, error = function(e) { # 오류 발생시 해당 종목명을 출력하고 다음 루프로 이동 warning(paste0(&quot;Error in Ticker: &quot;, name)) }) # 다운로드 받은 파일을 생성한 폴더 내 csv 파일로 저장 write.csv(price, paste0(&#39;data/KOR_price/&#39;, name, &#39;_price.csv&#39;)) # 타임슬립 적용 Sys.sleep(2) } 위의 코드에서 추가된 점은 다음과 같습니다. 페이지 오류, 통신 오류 등 오류가 발생할 경우 for loop 구문은 멈추어 버리며, 전체 데이터를 처음부터 다시 받는 일은 매우 귀찮은 작업입니다. 따라서 tryCatch() 함수를 이용해 오류가 발생 시 해당 티커를 출력한 후 다음 loop로 넘어가게 합니다. 또한 오류가 발생했을 시에는 xts() 함수를 통해 만들어 둔 빈 데이터를 저장하게 됩니다. 위의 코드가 모두 돌아가는데는 수시간이 걸리며, 작업이 끝난 후 data/KOR_price 폴더를 확인해보면, 전 종목 주가가 csv 형태로 저장되어 있게 됩니다. 6.2 재무제표 및 가치지표 크롤링 주가와 더불어 재무제표와 가치지표 역시 투자에 있어 핵심이 되는 데이터입니다. 해당 데이터 역시 구할 수 잇는 여러 사이트가 있지만, 국내의 데이터 제공업체인 FnGuide에서 운영하는 Company Guide 홈페이지17에서 손쉽게 구할 수 있습니다. 6.2.1 재무제표 다운로드 먼저 개별종목의 재무제표를 탭을 선택하면 포괄손익계산서, 재무상태표, 현금흐름표 항목이 보이게 되며, 티커에 해당하는 A005930 뒤의 주소는 불필요한 내용이므로, 이를 제거한 주소로 접속하도록 합니다. A 뒤의 6자리 티커만 변경할 경우, 해당 종목의 재무제표 페이지로 이동하게 됩니다. http://comp.fnguide.com/SVO2/ASP/SVD_Finance.asp?pGB=1&amp;gicode=A005930 우리가 원하는 재무제표 항목들은 모두 테이블 형태로 제공되고 있으므로, html_table() 함수를 이용하여 손쉽게 추출할 수 있습니다. library(httr) library(rvest) ifelse(dir.exists(&#39;data/KOR_fs&#39;), FALSE, dir.create(&#39;data/KOR_fs&#39;)) Sys.setlocale(&quot;LC_ALL&quot;, &quot;English&quot;) url = &#39;http://comp.fnguide.com/SVO2/ASP/SVD_Finance.asp?pGB=1&amp;gicode=A005930&#39; data = GET(url) data = data %&gt;% read_html() %&gt;% html_table() Sys.setlocale(&quot;LC_ALL&quot;, &quot;Korean&quot;) lapply(data, function(x) {head(x, 3)}) 먼저 data 폴더 내에 KOR_fs 폴더를 생성해줍니다. Sys.setlocale() 함수를 통해 로케일 언어를 영어로 설정 해줍니다. url을 입력한 후, GET() 함수를 통해 페이지 내용을 받아옵니다. read_html() 함수를 통해 html 내용을 읽어오며, html_table() 함수를 통해 테이블 내용만을 추출합니다. 로케일 언어를 다시 한글로 설정 해줍니다. 위의 과정을 거치면 data 변수에는 총 리스트 형태로 총 6개의 테이블이 들어오게 되며, 그 내용은 다음과 같습니다. Table 6.1: 재무제표 테이블 내역 순서 내용 1 포괄손익계산서 (연간) 2 포괄손익계산서 (분기) 3 재무상태표 (연간) 4 재무상태표 (분기) 5 현금흐름표 (연간) 6 현금흐름표 (분기) 이 중 연간 기준 재무제표에 해당하는 첫번째, 세번째, 다섯번째 테이블을 선택한 후, 클랜징 작업을 해주도록 하겠습니다. data_IS = data[[1]] data_BS = data[[3]] data_CF = data[[5]] print(names(data_IS)) ## [1] &quot;IFRS(연결)&quot; &quot;2016/12&quot; &quot;2017/12&quot; &quot;2018/12&quot; &quot;2019/03&quot; ## [6] &quot;전년동기&quot; &quot;전년동기(%)&quot; data_IS = data_IS[, 1:(ncol(data_IS)-2)] 포괄손익계산서 테이블(data_IS)에는 전년동기, 전년동기(%) 열이 존재하며, 통일성을 위해 이를 삭제해주었습니다. 이제 테이블을 묶은 후 클랜징을 해주도록 하겠습니다. data_fs = rbind(data_IS, data_BS, data_CF) data_fs[, 1] = gsub(&#39;계산에 참여한 계정 펼치기&#39;, &#39;&#39;, data_fs[, 1]) data_fs = data_fs[!duplicated(data_fs[, 1]), ] rownames(data_fs) = NULL rownames(data_fs) = data_fs[, 1] data_fs[, 1] = NULL data_fs = data_fs[, substr(colnames(data_fs), 6,7) == &quot;12&quot;] 먼저 rbind() 함수를 이용하여 세 테이블을 행으로 묶은 뒤 저장합니다. 첫번째 열인 계정명에는 ‘계산에 참여한 계정 펼치기’ 라는 글자가 들어간 항목이 존재합니다. 이는 페이지 내에서 펼치기 역할을 하는 (+) 항목에 해당하며, gsub() 함수를 이용해 해당 글자를 삭제해 줍니다. 중복되는 계정명이 다수 존재하며, 이는 대부분 불필요한 항목입니다. !duplicated() 함수를 사용해 중복되지 않는 계정명만을 선택해 줍니다. 행이름을 초기화 한 후, 첫번째 열의 계정명을 행이름으로 변경합니다. 그 후 첫번째 열은 삭제해주도록 합니다. 간혹 12월 결산법인이 아닌 종목, 혹은 연간 재무제표임에도 불구하고 분기 재무제표가 들어와 있는 경우가 있습니다. 비교의 통일성을 위해 substr() 함수를 이용하여 끝 글자가 12인 열, 즉 12월 결산 데이터만을 선택해 줍니다. print(head(data_fs)) ## 2016/12 2017/12 2018/12 ## 매출액 2,018,667 2,395,754 2,437,714 ## 매출원가 1,202,777 1,292,907 1,323,944 ## 매출총이익 815,890 1,102,847 1,113,770 ## 판매비와관리비 523,484 566,397 524,903 ## 인건비 59,763 67,972 64,514 ## 유무형자산상각비 10,018 13,366 14,477 str(data_fs) ## &#39;data.frame&#39;: 236 obs. of 3 variables: ## $ 2016/12: chr &quot;2,018,667&quot; &quot;1,202,777&quot; &quot;815,890&quot; &quot;523,484&quot; ... ## $ 2017/12: chr &quot;2,395,754&quot; &quot;1,292,907&quot; &quot;1,102,847&quot; &quot;566,397&quot; ... ## $ 2018/12: chr &quot;2,437,714&quot; &quot;1,323,944&quot; &quot;1,113,770&quot; &quot;524,903&quot; ... 데이터를 확인해보면 연간 기준 재무제표가 정리되었습니다. 그러나 데이터에 콤마가 문자형이므로, 이를 숫자형으로 변경해주어야 합니다. library(readr) library(stringr) data_fs = sapply(data_fs, function(x) { str_replace_all(x, &#39;,&#39;, &#39;&#39;) %&gt;% as.numeric() }) %&gt;% data.frame(., row.names = rownames(data_fs)) print(head(data_fs)) ## X2016.12 X2017.12 X2018.12 ## 매출액 2018667 2395754 2437714 ## 매출원가 1202777 1292907 1323944 ## 매출총이익 815890 1102847 1113770 ## 판매비와관리비 523484 566397 524903 ## 인건비 59763 67972 64514 ## 유무형자산상각비 10018 13366 14477 str(data_fs) ## &#39;data.frame&#39;: 236 obs. of 3 variables: ## $ X2016.12: num 2018667 1202777 815890 523484 59763 ... ## $ X2017.12: num 2395754 1292907 1102847 566397 67972 ... ## $ X2018.12: num 2437714 1323944 1113770 524903 64514 ... sapply() 함수를 이용해 각 열에 stringr 패키지의 str_replace_allr() 함수를 적용하여 콤마(,)를 제거한 후, as.numeric() 함수를 통해 숫자형 데이터로 변경합니다. data.frame() 함수를 이용해 데이터프레임 형태로 만들어주며, 행이름은 기존 내용을 그대로 유지해줍니다. 정리된 데이터를 출력해보면 문자형이던 데이터가 숫자형으로 변경되었습니다. write.csv(data_fs, &#39;data/KOR_fs/005930_fs.csv&#39;) data 폴더의 KOR_fs 폴더 내에 티커_fs.csv 이름으로 저장해주도록 합니다. 6.2.2 가치지표 계산하기 위에서 구한 재무제표 데이터를 이용해 가치지표를 계산할 수 있습니다. 흔히 사용되는 가치지표는 PER, PBR, PCR, PSR 이며 분자는 주가, 분모는 재무제표 데이터가 사용됩니다. Table 6.2: 가치지표의 종류 순서 분모 PER Earnings (순이익) PBR Book Value (순자산) PCR Cashflow (영업활동현금흐름) PSR Sales (매출액) 위에서 구한 재무제표 항목에서 분모 부분에 해당하는 데이터만 선택하도록 하겠습니다. ifelse(dir.exists(&#39;data/KOR_value&#39;), FALSE, dir.create(&#39;data/KOR_value&#39;)) ## [1] FALSE value_type = c(&#39;지배주주순이익&#39;, &#39;자본&#39;, &#39;영업활동으로인한현금흐름&#39;, &#39;매출액&#39;) value_index = data_fs[match(value_type, rownames(data_fs)), ncol(data_fs)] print(value_index) ## [1] 438909 2477532 670319 2437714 먼저 data 폴더 내에 KOR_value 폴더를 생성해줍니다. 분모에 해당하는 항목을 저장한 후, match() 함수를 이용하여 해당 항목이 위치하는 지점을 찾아 데이터를 선택해줍니다.ncol() 함수를 이용하여 가장 우측, 즉 최근년도 재무제표 데이터를 선택해줍니다. 다음으로 분자 부분에 해당하는 현재 주가를 수집해야 합니다. 이 역시 Company Guide 접속화면에서 구할 수 있으며, 불필요한 부분을 제거한 url은 다음과 같습니다. http://comp.fnguide.com/SVO2/ASP/SVD_main.asp?pGB=1&amp;gicode=A005930 위의 주소 역시 A 뒤의 6자리 티커만 변경할 경우, 해당 종목의 스냅샷 페이지로 이동하게 됩니다. Figure 2.8: Company Guide 스냅샷 화면 주가추이 부분에 우리가 원하는 현재 주가가 있으므로, 해당 데이터를 크롤링 하면 됩니다. 이처럼 크롤링하고자 하는 데이터가 하나 혹은 소수 일때는 html 구조를 모두 분해한 후 데이터를 추출하는 것 보다 Xpath를 이용하는 것이 훨씬 효율적입니다. Xpath란 XML 중 특정 값의 태그나 속성을 찾기 쉽게 만든 주소라 생각하면 됩니다. 예를 들어 R 프로그램이 저장된 곳을 윈도우 탐색기를 이용하여 이용할 경우 C:\\Program Files\\R\\R-3.4.1 형태의 주소를 보이며, 이는 윈도우의 path 문법입니다. XML 역시 이와 동일한 개념의 XPath가 존재합니다. 웹페이지에서 Xpath를 찾는 법은 다음과 같습니다. Figure 5.2: Company Guide 스냅샷 화면 먼저 크롤링하고자 하는 내용에 마우스를 올린 채 우클릭 → 검사를 누르면, 개발자도구 화면이 열리며 해당 지점의 html 부분이 선택됩니다.그 후 html 화면에서 우클릭 → Copy → Copy Xpath를 선택하면, 해당 지점의 xpath가 복사됩니다. //*[@id=&quot;svdMainChartTxt11&quot;] 위에서 구한 xpath를 이용하여 주가 데이터를 크롤링하도록 하겠습니다. url = &#39;http://comp.fnguide.com/SVO2/ASP/SVD_main.asp?pGB=1&amp;gicode=A005930&#39; data = GET(url) price = read_html(data) %&gt;% html_node(xpath = &#39;//*[@id=&quot;svdMainChartTxt11&quot;]&#39;) %&gt;% html_text() %&gt;% parse_number() print(price) ## [1] 44600 먼저 url을 입력한 후, GET() 함수를 이용하여 데이터를 불러옵니다. read_html() 함수를 이용해 html 데이터를 불러온 후, html_node() 함수 내에 위에서 구한 xpath를 입력하여, 해당 지점의 데이터를 추출합니다. html_text() 함수를 통해 텍스트 데이터만을 추출하며, parse_number() 함수를 적용합니다. 해당 함수는 문자형 데이터에서 콤마와 같은 불필요한 문자를 제거한 후, 숫자형 데이터로 변경해줍니다. 이처럼 xpath를 이용할 경우 태그나 속성을 분해하지 않고도 원하는 지점의 데이터를 크롤링할 수 있습니다. 가치지표를 계산하기 위해서는 발행주식수 역시 필요합니다. 예를 들어 PER를 계산하는 방법은 다음과 같습니다. \\[ PER = Price / EPS = 주가 / 주당순이익\\] 주당순이익의 경우 순이익을 전체 주식수로 나눈값이므로, 해당 값의 계산을 위해 전체 주식수 역시 크롤링해야 합니다. 해당 데이터 역시 웹페이지에 존재하므로 위에서 주가를 크롤링한 방법과 동일하게 구할 수 있으며, xpath는 다음과 같습니다. //*[@id=&quot;svdMainGrid1&quot;]/table/tbody/tr[7]/td[1] 이를 이용해 발행주식수 중 보통주를 선택하는 방법은 다음과 같습니다. share = read_html(data) %&gt;% html_node(xpath = &#39;//*[@id=&quot;svdMainGrid1&quot;]/table/tbody/tr[7]/td[1]&#39;) %&gt;% html_text() print(share) ## [1] &quot;5,969,782,550/ 822,886,700&quot; read_html() 함수와 html_node() 함수를 이용해, html 내에서 xpath에 해당하는 데이터를 추출합니다. 그 후, html_text() 함수를 통해 텍스트 부분만 추출하도록 합니다. 해당 과정을 거치면 보통주/우선주의 형태로 발행주식주가 저장되어 있습니다. 이 중 우리가 원하는 데이터는 ‘/’ 앞에 위치한 보통주 발행주식수 입니다. share = share %&gt;% strsplit(&#39;/&#39;) %&gt;% unlist() %&gt;% .[1] %&gt;% parse_number() print(share) ## [1] 5969782550 strsplit() 함수를 통해 ’/’를 기준으로 데이터를 나누어주며, 해당 결과는 리스트 형태로 저장이 됩니다. unlist() 함수를 통해 리스트를 벡터 형태로 변환합니다. .[1]을 통해 보통주 발행주식수인 첫번째 데이터를 선택합니다. parse_number() 함수를 통해 문자형 데이터를 숫자형으로 변환해 줍니다. 재무데이터, 현재 주가, 발행주식수를 이용하여 가치지표를 계산해보도록 하겠습니다. data_value = price / (value_index * 100000000/ share) names(data_value) = c(&#39;PER&#39;, &#39;PBR&#39;, &#39;PCR&#39;, &#39;PSR&#39;) data_value[data_value &lt; 0] = NA print(data_value) ## PER PBR PCR PSR ## 6.066230 1.074667 3.972024 1.092221 분자에는 현재 주가를 입력하며, 분모에는 재무 데이터를 보통주 발행주식수로 나눈 값을 입력합니다. 단, 주가는 원 단위, 재무 데이터는 억 단위이므로, 둘 간의 단위를 동일하게 맞춰주기 위해 분모에 억을 곱해 줍니다. 또한 가치지표가 음수인 경우는NA로 변경해주도록 합니다. 결과를 확인해보면 4가지 가치지표가 잘 계산되었습니다.18 write.csv(data_value, &#39;data/KOR_value/005930_value.csv&#39;) data 폴더의 KOR_value 폴더 내에 티커_value.csv 이름으로 저장해주도록 합니다. 6.2.3 전 종목 재무제표 및 가치지표 다운로드 위의 코드에서 for loop 구문을 이용하여 url 중 6자리 티커에 해당하는 값만 변경해주면 모든 종목의 재무제표를 다운로드 받고, 이를 바탕으로 가치지표를 계산할 수 있습니다. 해당 코드는 다음과 같습니다. library(stringr) library(httr) library(rvest) library(stringr) library(readr) KOR_ticker = read.csv(&#39;data/KOR_ticker.csv&#39;, row.names = 1) KOR_ticker$&#39;종목코드&#39; = str_pad(KOR_ticker$&#39;종목코드&#39;, 6, side = c(&#39;left&#39;), pad = &#39;0&#39;) ifelse(dir.exists(&#39;data/KOR_fs&#39;), FALSE, dir.create(&#39;data/KOR_fs&#39;)) ifelse(dir.exists(&#39;data/KOR_value&#39;), FALSE, dir.create(&#39;data/KOR_value&#39;)) for(i in 1 : nrow(KOR_ticker) ) { data_fs = c() data_value = c() name = KOR_ticker$&#39;종목코드&#39;[i] # 오류 발생 시 이를 무시하고 다음 루프로 진행 tryCatch({ # url 생성 url = paste0(&quot;http://comp.fnguide.com/SVO2/ASP/SVD_Finance.asp?pGB=1&amp;gicode=A&quot;, name) # 이 후 과정은 위와 동일함 # 데이터 다운로드 Sys.setlocale(&quot;LC_ALL&quot;, &quot;English&quot;) # 테이블 추출 data = GET(url) %&gt;% read_html() %&gt;% html_table() Sys.setlocale(&quot;LC_ALL&quot;, &quot;Korean&quot;) # 3개 재무제표를 하나로 합치기 data_IS = data[[1]] data_BS = data[[3]] data_CF = data[[5]] data_IS = data_IS[, 1:(ncol(data_IS)-2)] data_fs = rbind(data_IS, data_BS, data_CF) # 데이터 클랜징 data_fs[, 1] = gsub(&#39;계산에 참여한 계정 펼치기&#39;, &#39;&#39;, data_fs[, 1]) data_fs = data_fs[!duplicated(data_fs[, 1]), ] rownames(data_fs) = NULL rownames(data_fs) = data_fs[, 1] data_fs[, 1] = NULL # 12월 재무제표만 선택 data_fs = data_fs[, substr(colnames(data_fs), 6,7) == &quot;12&quot;] data_fs = sapply(data_fs, function(x) { str_replace_all(x, &#39;,&#39;, &#39;&#39;) %&gt;% as.numeric() }) %&gt;% data.frame(., row.names = rownames(data_fs)) # 가치지표 분모부분 value_type = c(&#39;지배주주순이익&#39;, &#39;자본&#39;, &#39;영업활동으로인한현금흐름&#39;, &#39;매출액&#39;) # 해당 재무데이터만 선택 value_index = data_fs[match(value_type, rownames(data_fs)), ncol(data_fs)] # Snapshot 페이지 불러오기 url = paste0(&quot;http://comp.fnguide.com/SVO2/ASP/SVD_Main.asp?pGB=1&amp;gicode=A&quot;, name) data = GET(url) # 현재 주가 크롤링 price = read_html(data) %&gt;% html_node(xpath = &#39;//*[@id=&quot;svdMainChartTxt11&quot;]&#39;) %&gt;% html_text() %&gt;% parse_number() # 보통주 발행장주식수 크롤링 share = read_html(data) %&gt;% html_node(xpath = &#39;//*[@id=&quot;svdMainGrid1&quot;]/table/tbody/tr[7]/td[1]&#39;) %&gt;% html_text() %&gt;% strsplit(&#39;/&#39;) %&gt;% unlist() %&gt;% .[1] %&gt;% parse_number() # 가치지표 계산 data_value = price / (value_index * 100000000/ share) names(data_value) = c(&#39;PER&#39;, &#39;PBR&#39;, &#39;PCR&#39;, &#39;PSR&#39;) data_value[data_value &lt; 0] = NA }, error = function(e) { # 오류 발생시 해당 종목명을 출력하고 다음 루프로 이동 data_fs &lt;&lt;- NA data_value &lt;&lt;- NA warning(paste0(&quot;Error in Ticker: &quot;, name)) }) # 다운로드 받은 파일을 생성한 각각의 폴더 내 csv 파일로 저장 # 재무제표 저장 write.csv(data_fs, paste0(&#39;data/KOR_fs/&#39;, name, &#39;_fs.csv&#39;)) # 가치지표 저장 write.csv(data_value, paste0(&#39;data/KOR_value/&#39;, name, &#39;_value.csv&#39;)) # 2초간 타임슬립 적용 Sys.sleep(2) } 전종목 주가 데이터를 받는 과정과 동일하게, KOR_ticker.csv 파일을 불러온 후 for loop을 통해 i값이 변함에 따라 티커를 변경해가며 모든 종목의 재무제표 및 가치지표를 다운로드 받습니다. tryCatch() 함수를 이용해 오류가 발생 시 NA로 이루어진 빈 데이터를 저장한 후, 다음 루프로 넘어가게 됩니다. data/KOR_fs 폴더에는 전 종목의 재무제표 데이터가, data/KOR_value 폴더에는 전 종목의 가치지표 데이터가 csv 형태로 저장되어 있게 됩니다. 6.3 야후 파이낸스 데이터 구하기 크롤링을 이용하여 데이터를 수집할 경우 주의해야 할 점은, 웹페이지 구조가 변경되는 경우입니다. 웹페이지의 형태가 변경될 경우 이에 맞춰 코드를 다시 짜야합니다. 그러나 최악의 경우는 웹사이트가 폐쇄되는 경우입니다. 실제로 투자자들이 많이 사용하던 구글 파이낸스가 2018년 서비스를 중단함에 따라 이를 이용하던 사이트 및 패키지에서 오류가 발생하기도 했습니다. 이러한 상황에 대비하여 데이터를 구할수 있는 예비 사이트를 알아두어 테스트 코드를 작성해둘 필요가 있으며, 이를 위해 야후 파이낸스에서 데이터 구하는 법을 살펴보도록 하겠습니다. 주가 데이터의 경우 getSymbols() 함수를 통해 야후 파이낸스에서 제공하는 API를 사용하여 주가를 다운로드 받는 방법을 이미 살펴보았으며, 웹페이지에서 재무제표를 크롤링하는법 및 가치지표를 계산하는 법에 대해 알아보도록 하겠습니다. 6.3.1 재무제표 다운로드 Figure 6.1: 야후 파이낸스 재무제표 먼저 야후 파이낸스에 접속하여 삼성전자 티커에 해당하는 005930.KS를 입력합니다. 그 후, 재무제표 데이터에 해당하는 Financials 항목을 선택합니다. 손익계산서(Income Statement), 재무상태표(Balance Sheet), 현금흐름표(Cash Flow) 총 3개 지표가 있으며, 각각의 url은 다음과 같습니다. Income Statement: https://finance.yahoo.com/quote/005930.KS/financials?p=005930.KS Balance Sheet: https://finance.yahoo.com/quote/005930.KS/balance-sheet?p=005930.KS Cash Flow: https://finance.yahoo.com/quote/005930.KS/cash-flow?p=005930.KS 각 페이지에서 Xpath를 이용하여 재무제표에 해당하는 테이블 부분만을 선택하여 추출할 수 있으며, 3개 페이지의 해당 Xpath는 모두 아래와 같이 동일합니다. //*[@id=&quot;Col1-1-Financials-Proxy&quot;]/section/div[3]/table 위의 정보를 이용하여 재무제표를 다운로드 받는 과정은 다음과 같습니다. library(httr) library(rvest) url_IS = &#39;https://finance.yahoo.com/quote/005930.KS/financials?p=005930.KS&#39; url_BS = &#39;https://finance.yahoo.com/quote/005930.KS/balance-sheet?p=005930.KS&#39; url_CF = &#39;https://finance.yahoo.com/quote/005930.KS/cash-flow?p=005930.KS&#39; yahoo_finance_xpath = &#39;//*[@id=&quot;Col1-1-Financials-Proxy&quot;]/section/div[3]/table&#39; data_IS = GET(url_IS) %&gt;% read_html() %&gt;% html_node(xpath = yahoo_finance_xpath) %&gt;% html_table() data_BS = GET(url_BS) %&gt;% read_html() %&gt;% html_node(xpath = yahoo_finance_xpath) %&gt;% html_table() data_CF = GET(url_CF) %&gt;% read_html() %&gt;% html_node(xpath = yahoo_finance_xpath) %&gt;% html_table() data_fs = rbind(data_IS, data_BS, data_CF) print(head(data_fs)) ## X1 X2 X3 ## 1 Revenue 12/31/2018 12/31/2017 ## 2 Total Revenue 243,771,415,000 239,575,376,000 ## 3 Cost of Revenue 132,394,411,000 129,290,661,000 ## 4 Gross Profit 111,377,004,000 110,284,715,000 ## 5 Operating Expenses Operating Expenses Operating Expenses ## 6 Research Development 18,354,080,000 16,355,612,000 ## X4 X5 ## 1 12/31/2016 12/31/2015 ## 2 201,866,745,000 200,653,482,000 ## 3 120,277,715,000 123,482,118,000 ## 4 81,589,030,000 77,171,364,000 ## 5 Operating Expenses Operating Expenses ## 6 14,111,381,000 13,705,695,000 위에서 구한 url을 저장해줍니다. GET() 함수를 통해 페이지 정보를 받아온 후, read_html() 함수를 통해 html 정보를 받아옵니다. html_node() 함수 내에서 위에서 구한 xpath를 이용해 테이블 부분의 html을 선택한 후, html_table()을 통해 테이블 형태만 추출합니다. 3개 페이지에 위의 내용을 동일하게 적용한 후, rbind()를 이용해 행으로 묶어줍니다. 다운로드 받은 데이터를 클랜징 작업을 해주도록 하며, 그 과정은 앞선 예시와 거의 비슷합니다. library(stringr) data_fs = data_fs[!duplicated(data_fs[, 1]), ] rownames(data_fs) = NULL rownames(data_fs) = data_fs[, 1] colnames(data_fs) = data_fs[1, ] data_fs = data_fs[-1, ] data_fs = data_fs[, substr(colnames(data_fs), 1,2) == &quot;12&quot;] data_fs = sapply(data_fs, function(x) { str_replace_all(x, &#39;,&#39;, &#39;&#39;) %&gt;% as.numeric() }) %&gt;% data.frame(., row.names = rownames(data_fs)) print(head(data_fs)) ## X12.31.2018 X12.31.2017 X12.31.2016 ## Total Revenue 243771415000 239575376000 201866745000 ## Cost of Revenue 132394411000 129290661000 120277715000 ## Gross Profit 111377004000 110284715000 81589030000 ## Operating Expenses NA NA NA ## Research Development 18354080000 16355612000 14111381000 ## Selling General and Administrative 32688565000 38947445000 37235161000 ## X12.31.2015 ## Total Revenue 200653482000 ## Cost of Revenue 123482118000 ## Gross Profit 77171364000 ## Operating Expenses NA ## Research Development 13705695000 ## Selling General and Administrative 36081636000 1.!duplicated() 함수를 사용해 중복되지 않는 계정명만을 선택해 줍니다. 4. 행이름을 초기화 한 후, 첫번째 열의 계정명을 행이름으로 변경합니다. 그 후 첫번째 열은 삭제해주도록 합니다. 3. 열이름으로 첫번째 행으로 변경한 후, 해당 행은 삭제해주도록 합니다. 4. substr() 함수를 이용하여 처음 두 글자가 ’12’인 열, 즉 12월 결산 데이터만을 선택해 줍니다. 5. sapply() 함수를 이용해 각 열에 stringr 패키지의 str_replace_allr() 함수를 적용하여 콤마(,)를 제거한 후, as.numeric() 함수를 통해 숫자형 데이터로 변경합니다. 6. data.frame() 함수를 이용해 데이터프레임 형태로 만들어주며, 행이름은 기존 내용을 그대로 유지해줍니다. 6.3.2 가치지표 계산하기 가치지표를 계산하는 과정도 앞선 예시와 거의 비슷합니다. value_type = c(&#39;Net Income Applicable To Common Shares&#39;, # Earnings &#39;Total Stockholder Equity&#39;, # Book Value &#39;Total Cash Flow From Operating Activities&#39;, # Cash Flow &#39;Total Revenue&#39;) # Sales value_index = data_fs[match(value_type, rownames(data_fs)), 1] print(value_index) ## [1] 43890877000 240068993000 67031863000 243771415000 먼저 분모에 해당하는 항목을 저장한 후, match() 함수를 이용하여 해당 항목이 위치하는 지점을 찾아 데이터를 선택해줍니다.기존 예제와 다른점은, 야후 파이낸스의 경우 최근년도 데이터가 가장 좌측에 위치하므로, 첫번째 열을 선택해 줍니다. 다음으로 현재 주가 및 상장주식수를 구해야 합니다. 해당 데이터는 Statistics 항목에서 구할 수 있습니다. 먼저 현재 주가를 크롤링 하는 방법입니다. url = &#39;https://finance.yahoo.com/quote/005930.KS/key-statistics?p=005930.KS&#39; data = GET(url) price = read_html(data) %&gt;% html_node(xpath = &#39;//*[@id=&quot;quote-header-info&quot;]/div[3]/div/div/span[1]&#39;) %&gt;% html_text() %&gt;% str_replace_all(&#39;,&#39;, &#39;&#39;) %&gt;% as.numeric() print(price) ## [1] 43750 해당 페이지 url을 저장 후, GET() 함수를 통해 페이지 정보를 받습니다. read_html() 함수를 이용해 데이터를 받고, html_node() 함수와 Xpath를 이용해 현재 주가에 해당하는 html을 구합니다. 주가의 경우 페이지 상단에서 확인할 수 있습니다. html_text() 함수를 이용해 텍스트 데이터만을 추출한 후, str_replace_all() 함수를 통해 콤마 부분을 삭제해 줍니다. 그리고 as.numeric()을 통해 숫자형 데이터로 변경해 줍니다. 이처럼 주가의 경우 상대적으로 쉽게 데이터를 구할 수 있습니다. 다음은 상장주식수 데이터를 크롤링하는 방법입니다. share_yahoo = read_html(data) %&gt;% html_node(xpath = &#39;//*[@id=&quot;Col1-0-KeyStatistics-Proxy&quot;]/section/div[2]/div[2]/div/div[2]/table/tbody/tr[3]/td[2]&#39;) %&gt;% html_text() print(share_yahoo) ## [1] &quot;5.97B&quot; 상장주식수의 경우 Shares Outstanding 부분에서 찾을 수 있습니다. 해당 지점의 Xpath를 이용해 데이터를 찾으면 5.97B가 추출됩니다. 이 중 숫자 뒤 알파벳 부분은 단위에 해당하는 부분이며, 각 문자 별 단위는 다음과 같습니다. Table 6.3: 발행주식수 단위 알파벳 단위 숫자 M 백만 (Million) 1,000,000 B 십억 (Billion) 1,000,000,000 T 일조 (Triliion) 1,000,000,000,000 따라서 알파벳을 해당하는 숫자로 변경한 뒤, 이를 앞의 숫자에 곱해주어야 제대로된 상장주식수가 계산됩니다. library(stringr) share_unit = str_match(share_yahoo, &#39;[a-zA-Z]&#39;) print(share_unit) ## [,1] ## [1,] &quot;B&quot; share_multiplier = switch(share_unit, &#39;M&#39; = { 1000000 }, &#39;B&#39; = { 1000000000 }, &#39;T&#39; = { 1000000000000 } ) print(share_multiplier) ## [1] 1e+09 먼저 str_match() 함수 내에서 정규표현식19을 사용하여 알파벳을 추출한 후, 이를 share_unit에 저장합니다. 그 후, switch 구문을 이용하여 해당 알파벳에 해당하는 숫자를 share_multiplier에 저장해줍니다. share_yahoo = share_yahoo %&gt;% str_match(&#39;[0-9.0-9]*&#39;) %&gt;% as.numeric() share_yahoo = share_yahoo * share_multiplier print(share_yahoo) ## [1] 5.97e+09 숫자 부분과 위에서 구한 단위 부분을 곱하여 최종 발행주식수를 구하도록 하겠습니다. 먼저 str_match() 함수 내에 정규표현식을 이용하여 숫자에 해당하는 부분만 추출한 후, as.numeric()을 통해 숫자 형태로 변경합니다. 그 후 단위에 해당하는 숫자를 곱해 최종값을 구하도록 합니다. 위에서 구한 재무데이터, 현재주가, 발행주식수를 이용하여 가치지표를 계산하도록 하겠습니다. data_value_yahoo = price / (value_index * 1000/ share_yahoo) names(data_value_yahoo) = c(&#39;PER&#39;, &#39;PBR&#39;, &#39;PCR&#39;, &#39;PSR&#39;) data_value_yahoo[data_value_yahoo &lt; 0] = NA print(data_value_yahoo) ## PER PBR PCR PSR ## 5.950838 1.087968 3.896468 1.071444 분자에는 주가를, 분모에는 재무 데이터를 보통주 발행주식수로 나눈 값을 입력합니다. 야후 파이낸스의 재무 데이터는 천원 단위이므로, 둘 간의 단위를 동일하게 맞춰주기 위해 분모에 천을 곱해 줍니다. 또한 가치지표가 음수인 경우는NA로 변경해주도록 합니다. 결과를 확인해보면 4가지 가치지표가 잘 계산되었습니다. Company Guide에서 구한 값과는 재무데이터의 차이로 인해 미세한 차이가 있지만, 이는 무시해도 될 정도입니다. Table 6.4: 두 사이트의 가치지표 비교 PER PBR PCR PSR Company Guide 6.07 1.07 3.97 1.09 야후 파이낸스 5.95 1.09 3.90 1.07 해당 방법 또한 url의 티커 부분만 변경하면 전 종목의 재무제표와 가치지표 데이터를 다운로드 받을 수 있습니다. 그러나 주의해야 할점은 코스피 종목은 끝이 .KS, 코스닥 종목은 끝이 .KQ가 되어야 합니다. 자세한 코드는 생략하도록 하겠습니다. https://finance.naver.com/item/fchart.nhn?code=005930↩ http://comp.fnguide.com/↩ 분모에 사용되는 재무데이터의 구체적인 항목과 발행주식수를 계산하는 방법의 차이로 인해 여러 업체에서 제공하는 가치지표와 다소 차이가 발생할 수 있습니다.↩ 특정한 규칙을 가진 문자열의 집합을 표현하는데 사용하는 형식 언어이며, 이는 본 책의 범위를 넘어가므로 설명은 생략하도록 합니다.↩ "],
["section-33.html", "Chapter 7 데이터 정리하기 7.1 주가 정리하기 7.2 재무제표 정리하기", " Chapter 7 데이터 정리하기 앞장에서는 API와 크롤링을 통하여 퀀트 투자에 필요한 주가, 재무제표, 가치지표를 수집하는 방법에 대해 배웠습니다. 이번 장에서는 각각 csv 파일로 저장된 데이터들을 하나로 정리하는 과정을 살펴보도록 하겠습니다. 7.1 주가 정리하기 주가의 경우 data/KOR_price 폴더 내 티커.csv 파일로 저장되어 있습니다. 해당 파일들을 불러온 후 데이터를 묶는 작업을 통해 하나의 파일로 합치는 방법에 대해 알아보도록 하겠습니다. library(stringr) library(xts) library(magrittr) KOR_ticker = read.csv(&#39;data/KOR_ticker.csv&#39;, row.names = 1) KOR_ticker$&#39;종목코드&#39; = str_pad(KOR_ticker$&#39;종목코드&#39;, 6, side = c(&#39;left&#39;), pad = &#39;0&#39;) price_list = list() for (i in 1 : nrow(KOR_ticker)) { name = KOR_ticker[i, &#39;종목코드&#39;] price_list[[i]] = read.csv(paste0(&#39;data/KOR_price/&#39;, name, &#39;_price.csv&#39;), row.names = 1) %&gt;% as.xts() } price_list = do.call(cbind, price_list) %&gt;% na.locf() colnames(price_list) = KOR_ticker$&#39;종목코드&#39; head(price_list[, 1:5]) ## 005930 000660 005380 068270 051910 ## 2017-05-23 NA NA NA 91678 289500 ## 2017-05-24 NA NA 164000 93149 290000 ## 2017-05-25 NA NA 165000 91972 296000 ## 2017-05-26 NA NA 163500 91972 304500 ## 2017-05-29 45620 57900 162000 91776 309500 ## 2017-05-30 44640 57400 164000 93933 306500 먼저 티커가 저장된 csv 파일을 불러온 후, 티커를 6자리로 맞춰주도록 합니다. 빈 리스트인 pirce_list를 생성합니다. for loop 구문을 이용해 종목별 가격 데이터를 불러온 후, as.xts()를 통해 시계열 형태로 데이터를 변경한 후 리스트에 저장합니다. do.call() 함수를 통해 리스트를 열의 형태로 묶습니다. 간혹 결측치가 발생할 수 있으므로, na.locf() 함수를 통해 결측치의 경우 전일 데이터를 사용하도록 합니다. 행 이름을 각 종목의 티커로 변경해 주도록 합니다. 해당 작업을 통해 각각 흩어져있던 가격 데이터가 하나의 데이터로 잘 묶어지게 되었습니다. write.csv(data.frame(price_list), &#39;data/KOR_price.csv&#39;) 마지막으로 해당 데이터를 data 폴더 내 KOR_price.csv 파일로 저장해주도록 합니다. 시계열 형태 그대로 저장할 경우 인덱스가 삭제되므로, 데이터프레임 형태로 변경한 후 저장하도록 합니다. 7.2 재무제표 정리하기 "]
]
